[{"uri":"https://dokang307.github.io/Internship_Report/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Vietnam Cloud Day Event Objectives Equip executive leadership with strategic insights to navigate the Generative AI era. Disseminate best practices for establishing a unified, scalable data foundation on AWS. Present the AI-Driven Development Lifecycle (AI-DLC) and its transformative role in software engineering. Examine core security principles for Generative AI and the evolution toward autonomous AI Agents. Speakers Eric Yeo ‚Äì Country GM, Vietnam, Cambodia, Laos \u0026amp; Myanmar, AWS Dr. Jens Lottner ‚Äì CEO, Techcombank Ms. Trang Phung ‚Äì CEO \u0026amp; Co-Founder, U2U Network Jaime Valles ‚Äì VP \u0026amp; GM APJ, AWS Panelists: Jeff Johnson, Vu Van (ELSA), Nguyen Hoa Binh (Nexttech), Dieter Botha (TymeX) AWS Specialists: Kien Nguyen, Jun Kai Loke, Tamelly Lim, Binh Tran, Taiki Dang, Michael Armentano Key Highlights 1. Strategic Leadership \u0026amp; Vision Keynote Sessions: Leaders from AWS, Techcombank, and U2U Network outlined their vision for cloud and AI adoption across the region. Executive Panel: A discussion titled \u0026ldquo;Navigating the GenAI Revolution\u0026rdquo; focused on building an innovation-centric culture, aligning AI with business strategy, and managing organizational change during AI integration. 2. Data Foundation \u0026amp; Roadmap Unified Data Foundation: This session detailed the construction of a robust infrastructure for data ingestion, storage, and governance‚Äîa mandatory prerequisite for effective AI workloads. GenAI Roadmap: AWS showcased its comprehensive vision and emerging tools designed to empower organizations to drive efficiency through GenAI. 3. The Future of Software Development AI-Driven Development Lifecycle (AI-DLC): Discussions centered on a shift where AI acts not merely as an assistant but as a central collaborator. This model combines AI execution with human oversight to accelerate innovation beyond traditional methodologies. 4. Security \u0026amp; Advanced Automation Securing GenAI: Security was addressed across three layers: infrastructure, models, and applications, emphasizing encryption, zero-trust architecture, and granular access controls. AI Agents: The event concluded with a focus on the shift from basic automation to Intelligent Agents‚Äîsystems capable of learning, adapting, and executing complex tasks autonomously. Key Takeaways Cultural Shift Adopting AI-DLC: Software development is evolving from human-led efforts with AI assistance to AI-centric collaboration, requiring teams to adapt their coding and testing approaches. Agents vs. Automation: There is a fundamental difference between static automation scripts and dynamic AI Agents that can make decisions based on changing inputs. Technical Pillars Data First: A unified and governed data foundation is critical for GenAI success. Security by Design: Security measures must be continuous and layered to ensure data confidentiality throughout the AI lifecycle. Applying to Work Assess Data Readiness: Evaluate the current AWS data infrastructure to ensure it meets the scalability and governance standards required for GenAI (referencing the Unified Data Foundation session). Explore AI Agents: Identify complex manual operations that are suitable for offloading to autonomous AI Agents rather than simple scripts. Adopt AI-DLC: Experiment with integrating AI tools more deeply into the development lifecycle, treating them as collaborators rather than just code completion utilities. Event Experience The summit offered a holistic perspective on the GenAI landscape, effectively balancing high-level strategy with technical depth.\nStrategic Insight: The panel featuring leaders from ELSA, Nexttech, and TymeX provided valuable real-world context on managing the cultural impact of AI. Technical Depth: The afternoon tracks were highly relevant, particularly the deep dives into AI-DLC and Securing GenAI, which directly align with our technical roadmap. Some event photos Add your event photos here\n"},{"uri":"https://dokang307.github.io/Internship_Report/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only, please do not copy verbatim for your report, including this warning.\nOptimizing SAP Operations with CloudWatch MCP server and Amazon Q CLI ‚Äì Part 3 _by Ferry Mulyadi and Vijay Sitaram | AUGUST 11, 2025 | in Amazon CloudWatch, Amazon Q, Amazon Q Developer, SAP on AWS, Technical How-to, Thought Leadership | Permalink | Share_\nIntroduction In today\u0026rsquo;s complex SAP environments, operational efficiency and rapid troubleshooting capabilities are key factors in ensuring business continuity. Following previous discussions on SAP observability (Part 1) and Amazon CloudWatch Application Insights features (Part 2), this third article introduces a powerful combination of tools that is changing how teams manage SAP systems: Amazon CloudWatch Model Context Protocol (MCP) Server and Amazon Q for command line (Q CLI).\nThis innovative solution addresses three major challenges in SAP operations:\nComplex command syntax slowing down operational progress Time-consuming root cause analysis requiring multiple tools and contexts The need for lean system health monitoring and reporting In this blog, we will:\nExplore how CloudWatch MCP Server and Q CLI collaborate to simplify SAP operations Present real-world use cases for creating automated system health reports Illustrate how natural language commands can optimize routine maintenance tasks Demonstrate the alignment of this solution with the AWS Well-Architected Framework for SAP Part four of the series will focus on maintenance planning, advanced root cause analysis, and a detailed cost review when deploying this solution.\nFigure 1. Architecture diagram of Amazon CloudWatch MCP Server and Amazon Q for CLI\nWhat is Amazon Q for CLI (Q CLI)? Amazon Q CLI delivers transformative value to AWS customers by integrating an AI assistant directly into the command line workflow. It allows users to express commands in natural language, which Q then translates into standard CLI syntax, significantly shortening the learning curve. The tool provides contextual command suggestions based on user intent, eliminating time-consuming documentation searches. When errors occur, Q provides clear explanations and troubleshooting steps, accelerating the resolution process.\nQ decomposes commands into easy-to-understand components, enhancing comprehension and skills. By keeping users within the terminal environment, Q optimizes workflows by eliminating interruptions caused by switching to external resources.\nThis productivity improvement helps reduce time spent searching for syntax examples and troubleshooting, allowing developers to focus on building rather than wrestling with command structures. Q\u0026rsquo;s interactivity accelerates learning, helping users master the AWS CLI faster. Ultimately, Amazon Q CLI transforms the command line experience from a potential barrier into an intuitive interface, making AWS services more accessible and efficient for customers of all expertise levels.\nFigure 2. Amazon Q CLI prompt interface\nWhat is Amazon CloudWatch Model Context Protocol (MCP) Server? The Model Context Protocol (MCP) is an open-source standard that revolutionizes how development tools communicate with AI models. By providing structured context about code, files, and projects, MCP enables AI assistants to provide precisely tailored recommendations. A practical application is the CloudWatch MCP Server, which integrates metrics and logs from CloudWatch directly into the development environment. As a result, developers can monitor and troubleshoot without leaving their IDE (Integrated Development Environment), optimizing operational workflows.\nThe AWS Labs CloudWatch MCP Server bridges the gap between AI troubleshooting agents and operational data. It enables AI-driven root cause analysis and recommendations, using CloudWatch remotely without needing custom API integrations.\nThe Server features 4 specialized tools:\nAlarm Based Troubleshooting: Identifies active alarms and analyzes historical patterns to find root causes, offering contextual remediation recommendations. Log Analyzer: Inspects CloudWatch log groups for anomalies and errors over specific time intervals. Metric Definition Analyzer: Provides descriptions of metrics, calculation methods, and recommended statistics. Alarm Recommendations: Suggests optimal alarm configurations with appropriate thresholds and evaluation periods. What are the tools in CloudWatch MCP Server? When a user enters a natural language prompt in Q CLI, a tool from the CloudWatch MCP Server is invoked to interact with AWS services to perform the task. You can start Q from the client by typing q and using /tools or /mcp to view the list of available tools and loaded MCP servers in the environment.\nFigure 3. Q CLI Server tools loaded in the client session\nFigure 4. CloudWatch MCP server loaded in the client session\nHow does this solution align with the Well-Architected Framework for SAP Lens? Lean SAP Operations: Q CLI improves SAP operational processes by allowing engineers to use natural language to execute AWS operations supporting SAP workloads. This aligns with the SAP Lens focus on automation and reducing manual effort in managing critical business systems, as CLI commands for SAP infrastructure can be generated through simple conversational requests. Accelerated SAP Troubleshooting: This combination reduces the mean time to resolution (MTTR) for SAP scenarios by providing contextual monitoring data and AI-assisted analysis of SAP components, thereby supporting rapid recovery from errors affecting business operations. Integrated Observability for SAP: The solution eliminates context switching by consolidating SAP-specific metrics, logs, and alarms directly into the client environment, supporting end-to-end monitoring across complex SAP systems. This integration helps teams observe SAP system behavior without workflow interruptions. SAP Change Impact Analysis: The tool helps teams understand how changes affect SAP system stability by providing immediate feedback on operational metrics across the entire SAP technology stack. SAP Resilience Verification: The tools simplify monitoring of SAP recovery mechanisms and fault isolation boundaries across distributed SAP systems, as well as testing the reliability of critical processes and transactions. Understanding SAP Architecture: Q CLI can explain relationships between AWS services and SAP applications based on CloudWatch data, giving your team additional perspective on SAP architectural reliability. Democratizing SAP Knowledge: The tool helps organizations \u0026ldquo;operate SAP as code.\u0026rdquo; Q CLI\u0026rsquo;s command-line explanation capabilities allow your team to operate efficiently on the SAP Landscape with sufficient information to make data-driven decisions. Prerequisites Deployment of an SAP S/4 HANA system with High Availability using AWS Launch Wizard for SAP Enablement of full-stack observability for SAP via Amazon CloudWatch Application Insights for SAP Installation of Amazon Q CLI following steps in Installing Amazon Q for command line Configuration of CloudWatch MCP Server on your machine following instructions in the AWS Labs CloudWatch MCP repository Configuration of AWS CLI on your machine following instructions in configuration settings for the AWS CLI Access to the solution is secured by AWS Identity and Access Management (IAM); also review Q CLI Security considerations and best practices as well as Security best practices for Systems Manager. Apply the least-privilege access model for SAP Basis and AWS Infrastructure Engineers.\nUse-case 1: Create Health Reports for SAP This use case will create a health report for the SAP HANA database and the SAP Central Services (SAP ASCS) cluster. Q CLI generates contextual command-line commands to detect Amazon EC2 instances and build the health report. In this scenario, Q CLI runs multiple commands to check the status of SAP Central Services and generate a complete report:\nEnter prompt to generate the cluster health report. Run aws ec2 describe-instances for instances belonging to the cluster. Run ssm crm_mon command to check the status of nodes in the cluster. Run ssm command to check pacemaker.log. Run ssm command to check corosync.log. Run ssm command to check corosync.log. Important Note: The prompts and responses below have been abbreviated. You can download Use-case 1 - Create Health Reports for SAP to view the full prompt and response from Q CLI.\nPrompt: Generate a health report for SAP HANA database, database cluster, and HSR.\ngenerate a health report for SAP Hana database, database cluster and HSR\n# SAP HANA Health Report\nExecutive Summary Based on the analysis of CloudWatch logs for the SAP HANA system in the SAPDemo environment, the overall health status is GOOD. The SAP HANA database, cluster infrastructure, and HANA System Replication (HSR) are all functioning properly after recovering from replication issues that occurred on July 11-12, 2025. ‚Ä¶‚Ä¶\nConclusion The SAP HANA database, cluster infrastructure, and HSR are currently healthy and functioning as expected. The system has demonstrated resilience by recovering from previous replication issues. Regular monitoring and proactive maintenance are recommended to maintain system stability and performance.\nPrompt: What is the current status of SAP ASCS ERS cluster nodes for sap application SAPDemo.\nwhat is the current status of SAP ASCS ERS cluster nodes for sap application SAPDemo.\nI\u0026rsquo;ll check the current status of the SAP ASCS and ERS cluster nodes for the SAPDemo SAP application.\nüõ†Ô∏è Using tool: executebash (trusted) ‚ãÆ ‚óè I will run the following shell command: aws ec2 describe-instances \u0026ndash;filters \u0026ldquo;Name=tag:Name,Values=_SAP-FIS-CS\u0026rdquo; \u0026ndash;query \u0026ldquo;Reservations[].Instances[].[InstanceId,Tags[?Key==\u0026lsquo;Name\u0026rsquo;].Value|[0],State.Name,InstanceType,PrivateIpAddress]\u0026rdquo; \u0026ndash;output table ‚ãÆ ‚Ü≥ Purpose: Checking EC2 instances that are part of the SAP *SAPDemocluster\n| DescribeInstances | +\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;+ | i-064751ba1b4c8340f | SAP-FIS-CS-sapclnode2 | running | c5a.xlarge | 10.0.155.180 | | i-0e08f883d5c3b3747 | SAP-FIS-CS-sapclnode1 | running | c5a.xlarge | 10.0.134.80 | +\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;+\n‚ãÆ ‚óè Completed in 1.737s\n‚Ä¶‚Ä¶\nConclusion The SAP ASCS/ERS cluster for the *SAPDemo application is functioning properly with the expected resource distribution: ‚Ä¢ ASCS (Central Services) is active on sapclnode2 ‚Ä¢ ERS (Enqueue Replication Server) is active on sapclnode1 ‚Ä¢ Both nodes are online and healthy ‚Ä¢ The cluster has quorum and is properly configured for high availability\nThis is the expected configuration for an SAP high-availability setup, where ASCS and ERS run on separate nodes to ensure enqueue replication functionality in case of a node failure. The cluster is operating normally and no immediate action is required.\nFor complex operations, Q CLI guides multi-step processes while MCP provides real-time feedback on system status.\nUse-case 2: Simplify SAP Operations The following use case aims to maintain SAP Application Servers. Q CLI streamlines daily SAP management by creating runbooks for routine maintenance tasks. SAP BASIS administrators can use Q CLI to manage administrative tasks while monitoring real-time system status via the CloudWatch MCP server during maintenance windows. In this scenario, Q CLI creates a task list as follows:\nPrompt to check App Server status. Run aws ec2 describe-instances to check the SAP application server. Run ssm ps command to check SAP dispatcher and dialog processes. Run ssm sapcontrol command to get system status. Summarize that the SAP application server is stopped. Prompt to start the SAP application server. Run ssm sapcontrol command to start the SAP application server. Run ssm sapcontrol command to check the status of processes. Run ssm ps command to check SAP dialog processes. Run ssm sapcontrol command to get system status. Summarize that the SAP application server has been started. Important Note: The prompts and responses below have been abbreviated. You can download Use-case 2 - Simplify SAP Operations to view the full prompt and response from Q CLI.\nPrompt: Get status of sap in application server sapbaseaas1. Do not start the server, only report on current status of SAP process.\nget status of sap in application server sapbaseaas1. Do not start the server, Only report on current status of SAP process.\nI\u0026rsquo;ll check the current status of SAP processes on the application server sapbaseaas1 without starting the server. üõ†Ô∏è Using tool: executebash (trusted) ‚ãÆ ‚óè I will run the following shell command: aws ec2 describe-instances \u0026ndash;filters \u0026ldquo;Name=tag:Name,Values=_sapbaseaas1\u0026rdquo; \u0026ndash;query \u0026ldquo;Reservations[].Instances[].[InstanceId,Tags[?Key==\u0026lsquo;Name\u0026rsquo;].Value|[0],State.Name,PrivateIpAddress]\u0026rdquo; \u0026ndash;output table ‚ãÆ ‚Ü≥ Purpose: Finding the instance ID for sapbaseaas1 ..\nNow I\u0026rsquo;ll check the current status of the SAP processes on the running sapbaseaas1 server: ‚Ä¶‚Ä¶..\nConclusion The SAP application server sapbaseaas1 is currently stopped. While the EC2 instance itself is running and the SAP support processes are active, the actual SAP application instance (dialog instance) is not running. No work processes are active, and the instance is showing as GRAY in the system status. The instance would need to be started to become operational and process SAP transactions.\nIn the next scenario, the SAP BASIS administrator will ask Amazon Q from the CLI to start an SAP application server, the associated SAP service, and report on the SAP application status. Q CLI\u0026rsquo;s execute_bash tool runs 8 SSM commands to execute the request.\nPrompt: Start SAP on the SAP application server sapbaseaas1.\nstart SAP on the SAP application server sapbaseaas1\nI\u0026rsquo;ll start the SAP application on the sapbaseaas1 server.\nüõ†Ô∏è Using tool: execute_bash (trusted) ‚ãÆ ‚óè I will run the following shell command: aws ssm send-command \u0026ndash;instance-ids i-0539ecbaf407c4886 \u0026ndash;document-name \u0026ldquo;AWS-RunShellScript\u0026rdquo; \u0026ndash;parameters \u0026lsquo;commands=[\u0026ldquo;sudo su - ecdadm -c \u0026quot;sapcontrol -nr 00 -function Start\u0026quot;\u0026rdquo;]\u0026rsquo; \u0026ndash;output json ‚ãÆ ‚Ü≥ Purpose: Starting SAP instance on sapbaseaas1\n..\nNow let\u0026rsquo;s check if the SAP instance has started successfully:\n‚Ä¶‚Ä¶..\nConclusion The SAP application server sapbaseaas1 is now fully operational. All required processes are running, and the instance is properly integrated with the SAP system. The instance is ready to handle user workloads and process SAP transactions.\nConclusion CloudWatch MCP Server delivers unified observability across the entire SAP landscape within the client environment. It consolidates SAP application logs, SAP HANA metrics, and AWS EC2 performance data. The system automatically correlates metrics across SAP components. Q CLI complements this by enabling natural language interaction with this rich dataset, making it easy for teams to generate system logs and simplify SAP system operations. This solution aligns with AWS Well-Architected Frameworks to help you increase productivity in operating and maintaining your SAP Landscape.\nIn future blog posts, we will discuss how you can leverage CloudWatch MCP Server and Q CLI for maintenance planning and accelerating root cause analysis for SAP, as well as diving into the cost considerations of the solution.\nAs a next step, try out this solution ‚Äî you can achieve at least a 6x gain in productivity. Stay tuned for our next innovations on Amazon CloudWatch, AWS Model Context Protocol Servers and Amazon Q for command line.\nJoin the SAP on AWS Discussion In addition to your customer account team and AWS support channels, we recently launched re:Post ‚Äì A Reimagined Q\u0026amp;A Experience for the AWS Community. Our AWS for SAP Solution Architecture team regularly monitors the AWS for SAP topic to discuss and answer questions to support customers and partners. If your question is not related to technical support, join the discussion at re:Post and contribute to the community knowledge base.\nAcknowledgments I would like to thank the following members for their contributions: Sreenath Middhi and Adam Hill.\nTAGS: #saponaws, AWS Systems Manager, CloudWatch, SAP\n"},{"uri":"https://dokang307.github.io/Internship_Report/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only, please do not copy verbatim for your report, including this warning.\nStreamlining SAP Operations with CloudWatch MCP Server and Amazon Q CLI ‚Äì Part 4 by Ferry Mulyadi and Vijay Sitaram | AUGUST 12, 2025 | in Amazon CloudWatch, Amazon Q, AWS Systems Manager, AWS Well-Architected, How-To, SAP on AWS, Technical How-to, Thought Leadership | Permalink | Share\nIntroduction Optimizing SAP operations on AWS requires effective monitoring, troubleshooting, and maintenance capabilities. Continuing our previous discussions on Amazon CloudWatch Application Insights in Part 1, how we used CloudWatch Application Insights to monitor SAP High Availability in Part 2, and Amazon CloudWatch Model Context Protocol (MCP) Server combined with Amazon Q for command line (Q CLI) in Part 3, this fourth article illustrates advanced real-world applications of these tools.\nWe will explore how this integration streamlines SAP maintenance planning and accelerates root cause analysis through practical use cases.\nThrough detailed examples, we will demonstrate how CloudWatch MCP Server and Q CLI work together to:\nOrchestrate planned maintenance events with minimal disruption. Accelerate issue diagnosis through intelligent log analysis. Provide context-aware recommendations for SAP operations. Reduce Mean Time To Resolution (MTTR) for SAP incidents. Use-case 3: Planned Maintenance Events for SAP In this case, we will execute a planned maintenance event within an SAP application cluster. Workflow integration allows engineering teams to diagnose issues without disruptive context switching, by viewing CloudWatch metrics directly within the IDE while using Q CLI to generate precise remediation commands.\nIn this scenario, Q CLI generated the following task list:\nPrompt to check the status of the App Server. Run aws ec2 describe-instances to check the SAP cluster status. Run ssm crm_mon command to check the ASCS and ERS cluster. Run ssm ps command to get the process status of ASCS and ERS. Run ssm sapcontrol command to get the process list. Run ssm command to check pacemaker.log. Run ssm crm command to check cluster resource status, constraints, and configuration. Summarize that ERS is disabled due to cluster misconfiguration. After prompt to check CloudWatch logs: Run describe_log_groups. Run analyze_log_groups. Run search execute_log_insights_query related to ASCS and ERS. Run search execute_log_insights_query related to current ERS issues. Summarize all log entries related to the ERS issue. After prompt to start the SAP ERS service: Run ssm crm command to check cluster resource status. Run ssm crm command to enable the ERS resource (failed as expected). Run ssm command to update resource configuration. Run ssm crm command to re-enable the ERS resource (after fixing configuration). Run ssm sapcontrol command to get the ASCS and ERS process list. Summarize that the ERS service has been started successfully. Important Note: The prompts and responses below have been abbreviated. You can download the Use-case 3 ‚Äì Planned maintenance events for SAP document to view the full prompts and responses from Q CLI.\nPrompt: Check the status of the SAP ASCS/ERS cluster for the SAPDemo SAP application without restarting any services.\n\u0026gt; I\u0026#39;ll check the status of the SAP ASCS/ERS cluster for the SAPDemo SAP application without restarting any services. üõ†Ô∏è Using tool: execute_bash (trusted) ‚ãÆ ‚óè I will run the following shell command: aws ec2 describe-instances --filters \u0026#34;Name=tag:Name,Values=*SAP-FIS-CS*\u0026#34; --query \u0026#34;Reservations[*].Instances[*].[InstanceId,Tags[?Key==\u0026#39;Name\u0026#39;].Value|[0],State.Name,InstanceType,PrivateIpAddress]\u0026#34; --output table ‚ãÆ ‚Ü≥ Purpose: Checking EC2 instances that are part of the SAP cluster ------------------------------------------------------------------------------------------- | DescribeInstances | +----------------------+-------------------------+----------+-------------+---------------+ | i-064751ba1b4c8340f | SAP-FIS-CS-sapclnode2 | running | c5a.xlarge | 10.0.155.180 | | i-0e08f883d5c3b3747 | SAP-FIS-CS-sapclnode1 | running | c5a.xlarge | 10.0.134.80 | +----------------------+-------------------------+----------+-------------+---------------+ ‚ãÆ ‚óè Completed in 1.666s ‚Ä¶‚Ä¶ ## Conclusion The SAP ASCS/ERS cluster for the SAPDemo application is partially operational: ‚Ä¢ The ASCS instance is running correctly on sapclnode2 ‚Ä¢ The ERS instance is intentionally disabled (target-role=Stopped in the configuration) ‚Ä¢ The virtual IP for ERS is still active on sapclnode1 ‚Ä¢ Both cluster nodes are online and the cluster has quorum This configuration appears to be intentional based on the cluster configuration, but it means that enqueue replication is not currently active. In a normal high-availability setup, both ASCS and ERS would be running to ensure enqueue table replication in case of a failover. In this next scenario, we use CloudWatch MCP Server to investigate pacemaker cluster logs to determine the status of SAP ASCS. CloudWatch MCP Server invokes tools such as describe_log_groups, analyze_log_groups, and execute_log_insights_query to perform the analysis.\nPrompt: Check CloudWatch logs related to SAPDemo application for SAP ASCS ERS Pacemaker cluster events, alarms and issues.\n\u0026gt; Check CloudWatch logs related to SAPDemo application for SAP ASCS ERS Pacemaker cluster events, alarms and issues. only use cloudwatch mcp server for this analysis. \u0026gt; I\u0026#39;ll analyze the CloudWatch logs related to the SAPDemo application for SAP ASCS ERS Pacemaker cluster events, alarms, and issues using only the CloudWatch MCP server. Let\u0026#39;s start by identifying the relevant log groups for the SAPDemo application: üõ†Ô∏è Using tool: describe_log_groups (trusted) from mcp server awslabscloudwatch_mcp_server ‚ãÆ ‚óè Running describe_log_groups with the param: ‚ãÆ { ‚ãÆ \u0026#34;arguments\u0026#34;: { ‚ãÆ \u0026#34;log_group_name_prefix\u0026#34;: \u0026#34;PACEMAKER\u0026#34;, ‚ãÆ \u0026#34;region\u0026#34;: \u0026#34;us-east-1\u0026#34; ‚ãÆ }, ‚ãÆ \u0026#34;name\u0026#34;: \u0026#34;describe_log_groups\u0026#34; ‚ãÆ } ‚ãÆ ‚óè Completed in 0.760s ‚Ä¶‚Ä¶ ### Analysis: This log data shows that the ERS component (rsc_sap_ECD_ERS11) has been successfully started on sapclnode1. The cluster first cleared previous failure records, then migrated the IP resource (rsc_ip_ECD_ERS11) from node 2 to node 1, and finally started the ERS service on node 1. The successful start is confirmed by the @rc-code=0 in the logs, indicating no errors. The monitoring operation is also successful, showing that the service is running properly. This aligns with our previous findings where we had identified issues with the ERS component and cleared CloudWatch alarms for it. The logs now confirm that the ERS service has been successfully started and is being monitored by the cluster, which explains why the service is now running despite previous alarms. Would you like me to analyze any specific aspect of these logs in more detail or provide recommendations for ongoing monitoring of the ERS component? Prompt: start SAP ERS service on node sapclnode1.\n\u0026gt; start SAP ERS services on node sapclnode1 \u0026gt; I\u0026#39;ll start the SAP ERS services on node sapclnode1. üõ†Ô∏è Using tool: execute_bash (trusted) ‚ãÆ ‚óè I will run the following shell command: aws ssm send-command --instance-ids i-0e08f883d5c3b3747 --document-name \u0026#34;AWS-RunShellScript\u0026#34; --parameters \u0026#39;commands=[\u0026#34;sudo crm resource status rsc_sap_ECD_ERS11\u0026#34;]\u0026#39; --output json ‚ãÆ ‚Ü≥ Purpose: Checking the current status of the ERS resource .. ‚Ä¶‚Ä¶ ## Conclusion The SAP ERS services have been successfully started on node sapclnode1. The enqueue replication is now active and functioning correctly, providing high availability for the SAP enqueue service. The cluster is now in a fully operational state with all resources running as expected. Use-case 4: Accelerate Root-Cause Analysis for SAP In this scenario, we have an SAP cluster where ERS encountered an error due to misconfiguration. With the integration between Q CLI and CloudWatch MCP Server, traditional diagnostic challenges in complex SAP systems ‚Äì where issues often propagate through multiple system layers ‚Äì are eliminated. Q CLI generated the following task list:\nPrompt to check the status of the App Server. Run aws ec2 describe-instances to check the SAP cluster status. Run ssm crm_mon command to check the ASCS and ERS cluster. Run ssm sapcontrol command to get the process list on node 1 and node 2. Run ssm crm command to check cluster resource status and logs. Run ssm command to check pacemaker.log. Run ssm crm command to cleanup resources. Run ssm crm_mon command to check cluster status. Run ssm sapcontrol command to get the process list on node 2. Summarize that the analysis results indicate the issue stems from a configuration problem. After \u0026ldquo;Prompt to troubleshoot this problem\u0026rdquo;: Run ssm command to check SAP System Logs for ASCS and ERS on node 2. Run ssm command to check SAP profiles of ASCS and ERS. Run ssm command to check hostname configuration in hosts file on node 2. Run ssm command to check IP addresses of node 1 and node 2. Run ssm command to update hosts file on node 1 and node 2. Run ssm crm_mon command to cleanup resources. Run ssm sapcontrol command to get the process list on node 2. Run ssm sapcontrol command to start ERS service, then check logs. Run ssm sapcontrol command to start ASCS, then check logs. Run ssm crm_mon command to stop cluster, then check status. Run ssm crm_mon command to cleanup cluster resources. Run ssm crm_mon command to start cluster, wait and check. Run ssm sapcontrol command to get ASCS process list on node 1. Run ssm sapcontrol command to start ASCS service, then check. Summarize recommendations to fix the issue. After \u0026ldquo;Prompt to start the SAP ERS service\u0026rdquo;: Run ssm crm command to stop ASCS and ERS cluster resources. Run ssm command to verify and update hosts file on node 1 and node 2. Run ssm command to check hostname resolution on node 1 and node 2 (first update failed, but retried with a different method). Run ssm command to check SAP Profiles of ASCS and ERS. Run ssm crm_mon command to cleanup cluster resources. Run ssm sapcontrol command to start SAP ASCS and ERS resources, then verify. Summarize that ASCS and ERS services have been started successfully. Important Note: The prompts and responses below have been abbreviated. You can download the Use-case 4 ‚Äì Accelerates Root-Cause Analysis for SAP document to view the full prompts and responses from Q CLI.\n[Figure 5. Failed status of SAP Central Services (ASCS/ERS)]\nPrompt: What is the current status of SAP ASCS ERS cluster nodes for SAP application SAPDemo.\n\u0026gt; what is the current status of SAP ASCS ERS cluster nodes for sap application SAPDemo. \u0026gt; I\u0026#39;ll check the current status of the SAP ASCS/ERS cluster nodes for the SAPDemo SAP application. üõ†Ô∏è Using tool: execute_bash (trusted) ‚ãÆ ‚óè I will run the following shell command: aws ec2 describe-instances --filters \u0026#34;Name=tag:Name,Values=*SAP-FIS-CS*\u0026#34; --query \u0026#34;Reservations[*].Instances[*].[InstanceId,Tags[?Key==\u0026#39;Name\u0026#39;].Value|[0],State.Name,InstanceType,PrivateIpAddress]\u0026#34; --output table ‚ãÆ ‚Ü≥ Purpose: Checking EC2 instances that are part of the SAP cluster ------------------------------------------------------------------------------------------- | DescribeInstances | +----------------------+-------------------------+----------+-------------+---------------+ | i-064751ba1b4c8340f | SAP-FIS-CS-sapclnode2 | running | c5a.xlarge | 10.0.155.180 | | i-0e08f883d5c3b3747 | SAP-FIS-CS-sapclnode1 | running | c5a.xlarge | 10.0.134.80 | +----------------------+-------------------------+----------+-------------+---------------+ ‚ãÆ ‚óè Completed in 1.815s ‚Ä¶‚Ä¶.. ## Current State Summary The SAP ASCS/ERS cluster for SAPDemo is currently in a degraded state: ‚Ä¢ The cluster infrastructure (corosync/pacemaker) is functioning correctly with quorum ‚Ä¢ Both nodes are online and communicating ‚Ä¢ The virtual IP addresses for both ASCS and ERS are properly assigned ‚Ä¢ However, the actual SAP services (ASCS and ERS) are failing to start ‚Ä¢ The application servers are showing YELLOW status, likely due to their dependency on the central services This indicates a problem with the SAP services themselves rather than with the cluster infrastructure. The services may be failing to start due to configuration issues, file system problems, or other SAP-specific issues that need to be investigated further. Prompt: Help me troubleshoot this problem.\n\u0026gt; help me troubleshoot this problem \u0026gt; I\u0026#39;ll help you troubleshoot the issues with the SAP ASCS/ERS cluster for the SAPDemo application. Based on the current state, both the ASCS and ERS services are failing to start properly, even though the cluster infrastructure itself is functioning correctly. Let\u0026#39;s take a systematic approach to troubleshooting: ## Step 1: Check SAP-specific error logs First, let\u0026#39;s check the SAP system logs to understand why the services are failing to start: üõ†Ô∏è Using tool: execute_bash (trusted) ‚ãÆ ‚óè I will run the following shell command: aws ssm send-command --instance-ids i-064751ba1b4c8340f --document-name \u0026#34;AWS-RunShellScript\u0026#34; --parameters \u0026#39;commands=[\u0026#34;sudo su - ecdadm -c \\\u0026#34;tail -50 /usr/sap/ECD/ASCS10/work/dev_ms\\\u0026#34;\u0026#34;]\u0026#39; --output json ‚ãÆ ‚Ü≥ Purpose: Checking SAP system logs for ASCS on node 2 .. \u0026gt; Now let\u0026#39;s check the ERS logs: ‚Ä¶‚Ä¶.. \u0026gt; Based on my troubleshooting, I\u0026#39;ve identified the root cause of the issues with your SAP ASCS/ERS cluster and can provide you with a comprehensive solution: # SAP ASCS/ERS Cluster Troubleshooting Report ## Root Cause Analysis After thorough investigation, I\u0026#39;ve identified that the primary issue is related to hostname resolution. The error message MsSInit: NihostToAddr rc=(NIEHOST_UNKNOWN) in the ASCS logs clearly indicates that the Message Server cannot resolve its hostname. ### Specific Issues Identified: 1. Hostname Resolution Problems: ‚Ä¢ The SAP profiles are configured to use the hostnames ecdascsnode and ecdersnode, but these hostnames were not properly defined in the /etc/hosts files on both nodes. ‚Ä¢ We\u0026#39;ve updated the hosts files, but the SAP services are still failing to start properly. 2. IP Address Configuration: ‚Ä¢ The virtual IPs (192.168.48.201 for ASCS and 192.168.48.202 for ERS) are not being properly managed by the cluster. ‚Ä¢ Both virtual IPs are currently assigned to node 1 (sapclnode1), which is causing conflicts. 3. SAP Profile Configuration: ‚Ä¢ The SAP profiles are using hostnames that don\u0026#39;t match the actual hostnames of the cluster nodes. ‚Ä¢ The ERS service is trying to connect to the ASCS service but failing with \u0026#34;Connection refused\u0026#34; errors. ## Solution Steps To fully resolve these issues, we need to take the following steps: ### 1. Fix the Hostname Resolution We\u0026#39;ve already updated the /etc/hosts files on both nodes with the correct mappings, but we need to ensure that the SAP services can properly resolve the hostnames. ### 2. Update the SAP Profiles The SAP profiles need to be updated to use the correct hostnames or IP addresses. This is a more involved task that requires careful editing of the SAP profiles. ### 3. Restart the Cluster Services in the Correct Order The cluster services need to be restarted in the correct order to ensure proper operation: 1. Stop all SAP services 2. Stop the cluster 3. Fix the configuration 4. Start the cluster 5. Start the SAP services ### 4. Verify DNS Resolution We need to ensure that DNS resolution is working correctly on both nodes. ## Recommended Actions Here\u0026#39;s a step-by-step plan to resolve the issues: 1. Stop the cluster resources: bash sudo crm resource stop rsc_sap_ECD_ASCS10 sudo crm resource stop rsc_sap_ECD_ERS11 2. Verify the hosts file configuration on both nodes: bash cat /etc/hosts Ensure that both nodes have the following entries: 127.0.0.1 localhost 10.0.134.80 sapclnode1.ec2.internal sapclnode1 10.0.155.180 sapclnode2.ec2.internal sapclnode2 192.168.48.201 ecdascsnode 192.168.48.202 ecdersnode 3. Test hostname resolution: bash ping -c 3 ecdascsnode ping -c 3 ecdersnode ping -c 3 sapclnode1 ping -c 3 sapclnode2 4. Update the SAP profiles if necessary: ‚Ä¢ Review the SAP profiles to ensure they\u0026#39;re using the correct hostnames ‚Ä¢ Consider updating the profiles to use IP addresses instead of hostnames if resolution continues to be an issue 5. Clean up the cluster resources: bash sudo crm resource cleanup rsc_sap_ECD_ASCS10 sudo crm resource cleanup rsc_sap_ECD_ERS11 6. Start the resources in the correct order: bash sudo crm resource start rsc_sap_ECD_ASCS10 # Wait for ASCS to fully start sudo crm resource start rsc_sap_ECD_ERS11 7. Verify the SAP services are running: bash sudo su - ecdadm -c \u0026#34;sapcontrol -nr 10 -function GetProcessList\u0026#34; sudo su - ecdadm -c \u0026#34;sapcontrol -nr 11 -function GetProcessList\u0026#34; ## Long-term Recommendations 1. Implement DNS Resolution: Consider implementing proper DNS resolution instead of relying on /etc/hosts files. 2. Review Cluster Configuration: Review the cluster configuration to ensure it\u0026#39;s properly set up for SAP ASCS/ERS high availability. 3. Monitor the Cluster: Set up proper monitoring for the cluster to detect and alert on issues. 4. Document the Configuration: Document the cluster configuration and troubleshooting steps for future reference. 5. Regular Testing: Regularly test failover scenarios to ensure the cluster is working correctly. By following these steps, you should be able to resolve the issues with your SAP ASCS/ERS cluster and ensure it\u0026#39;s properly configured for high availability. Prompt: Apply the above solution to fix the problem.\n\u0026gt; apply the above solution to fix the problem \u0026gt; I\u0026#39;ll apply the solution to fix the SAP ASCS/ERS cluster issues. Let\u0026#39;s follow the step-by-step plan: ## Step 1: Stop the cluster resources \u0026gt; I need to use SSM to execute these commands on the cluster nodes: üõ†Ô∏è Using tool: execute_bash (trusted) ‚ãÆ ‚óè I will run the following shell command: aws ssm send-command --instance-ids i-0e08f883d5c3b3747 --document-name \u0026#34;AWS-RunShellScript\u0026#34; --parameters \u0026#39;commands=[\u0026#34;sudo crm resource stop rsc_sap_ECD_ASCS10 \u0026amp;\u0026amp; sudo crm resource stop rsc_sap_ECD_ERS11\u0026#34;]\u0026#39; --output json ‚ãÆ ‚Ü≥ Purpose: Stopping the cluster resources using SSM .. \u0026gt; ## Step 2: Verify the hosts file configuration on both nodes ‚Ä¶‚Ä¶ \u0026gt; Great! The solution has been successfully applied and the SAP ASCS/ERS cluster is now working correctly. Here\u0026#39;s a summary of what we did: ## Summary of the Fix 1. Fixed Hostname Resolution: ‚Ä¢ Updated the /etc/hosts files on both nodes with the correct mappings for all hostnames. ‚Ä¢ Verified that hostname resolution was working correctly by pinging the virtual hostnames. 2. Cleaned Up and Restarted Cluster Resources: ‚Ä¢ Stopped the cluster resources to ensure a clean state. ‚Ä¢ Cleaned up any failed resource actions. ‚Ä¢ Started the resources in the correct order: ASCS first, then ERS. 3. Verified the Services: ‚Ä¢ Confirmed that both the ASCS and ERS services are now running correctly. ‚Ä¢ The system instance list shows all components are GREEN (running). ## Current Status ‚Ä¢ **Cluster Status**: All resources are running correctly. ‚Ä¢ **ASCS Status**: The Message Server and Enqueue Server are both running on node 2 (sapclnode2). ‚Ä¢ **ERS Status**: The Enqueue Replication Server is running on node 1 (sapclnode1). ‚Ä¢ **Virtual IPs**: The virtual IPs are correctly assigned to the appropriate nodes. ## Recommendations for Long-term Stability 1. Regular Monitoring: Set up monitoring for the cluster to detect any issues early. 2. Failover Testing: Regularly test failover scenarios to ensure the cluster works correctly. 3. Documentation: Document the cluster configuration and troubleshooting steps for future reference. 4. DNS Resolution: Consider implementing proper DNS resolution instead of relying on /etc/hosts files. The SAP ASCS/ERS cluster for the SAPDemo application is now functioning correctly. The issue was primarily related to hostname resolution, which we\u0026#39;ve fixed by properly configuring the hosts files on both nodes. [Figure 6. Final healthy status of SAP Central Services (ASCS/ERS) cluster]\nCost Considerations To deploy this solution, you need to consider the following factors:\nAmazon Q CLI Cost is shown in [this link]. We recommend using at least the Pro tier with 1,000 agentic requests per month ($19/month/user). Amazon CloudWatch Cost is shown in [this link]. In this experiment, we assume that the Free tier is sufficient. AWS Systems Manager Cost is shown in [this link]. For the HA experiment, assuming there are a minimum of 6 nodes to manage: $0.01137 \\text{ USD} \\times 6 \\text{ nodes} \\times 8 \\text{ hours} = 3.27456 \\text{ USD}$ No additional cost when using Amazon CloudWatch MCP Server. Estimated HA setup cost for SAP NetWeaver (calculated in us-east-1 region): 2 √ó m5.large = 148.16 USD (ASCS and ERS) 2 √ó m5.xlarge = 280.32 USD (2 App Servers) 2 √ó r5.2xlarge = 756.32 USD (SAP HANA Primary and Secondary) Total Estimated Cost = 19 USD + 3.27456 USD + 39.4933 USD ‚âà 61.76786 USD for a one-day experiment.\nConclusion CloudWatch MCP Server delivers unified observability across the entire SAP landscape within the customer environment. It consolidates SAP application logs, SAP HANA metrics, and AWS EC2 performance data. The system automatically correlates metrics across SAP components. Q CLI complements this capability by enabling natural language interaction with this rich dataset, allowing teams to easily investigate session response times, compare HANA memory consumption patterns, or identify waiting SAP processes during performance incidents.\nCloudWatch MCP Server helps troubleshoot complex SAP issues by identifying anomalies through pattern analysis in system logs and transaction errors. It links infrastructure alarms to application behavior. Additionally, it provides historical context from similar past incidents. Q CLI enhances this capability by interpreting SAP error codes, generating diagnostic commands based on observed patterns, suggesting root causes, and recommending relevant SAP transaction codes for deeper investigation.\nThis integration bridges the knowledge gap between SAP BASIS administrators and AWS infrastructure teams by creating a shared operational view of the SAP system and translating between SAP terminology and AWS concepts. This combination reduces mean time to resolution by eliminating context switching, providing intelligent analysis of SAP-specific patterns, and enabling natural language interaction with monitoring data ‚Äî leading to faster troubleshooting, higher reliability, and more efficient operations for critical SAP workloads on AWS.\nAs a next step, try applying this solution; you can achieve at least a 6x gain in productivity. Stay tuned for upcoming innovations regarding Amazon CloudWatch, AWS Model Context Protocol Servers, and Amazon Q for command line.\nJoin the SAP on AWS Discussion In addition to your customer account team and AWS Support channels, we recently launched re:Post ‚Äì A reimagined Q\u0026amp;A experience for the AWS community. Our AWS for SAP Solution Architecture team regularly monitors the AWS for SAP topic to discuss and answer questions to support customers and partners. If your question is not related to technical support, join the discussion at re:Post and contribute to the community knowledge base.\nAcknowledgments I would like to thank the following team members for their contributions: Sreenath Middhi and Adam Hill.\nTAGS: #SAP Netweaver, AWS Systems Manager, CloudWatch, SAP, SAP High Availability\n"},{"uri":"https://dokang307.github.io/Internship_Report/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nDeploy Okta as a Custom Identity Provider for AWS Transfer Family By: Yongki Kim and Tamelly Lim Date: AUGUST 11, 2025 Categories: AWS Transfer Family, Storage, Technical How-to, Permalink | Comments | Share\nAs projects and the number of users within an organization grow, managing granular access to data resources while adhering to security regulations becomes critically important. Multi-user environments require appropriate access delegation to ensure data privacy and security.\nHowever, managing access to shared files can be complex and time-consuming, especially as the user base expands and is distributed across multiple identity providers. Organizations need a scalable solution to enforce granular access controls, define which users can access sensitive data, and ensure appropriate access levels for shared files ‚Äî thereby reducing the risk of unauthorized access.\nAWS Transfer Family is a fully managed service that enables you to transfer files into and out of AWS storage services like Amazon S3 and Amazon EFS without managing any SFTP servers. The service also supports integration with identity providers like Okta, enabling end-user management for file exchange through an AWS managed SFTP server.\nIn a previous post on the Storage blog, we introduced the AWS Transfer Family Custom IdP solution, aimed at providing reusable components and examples to simplify setting up Transfer Family for common scenarios, as well as how to use it with Microsoft Active Directory authentication.\nIn this post, we will guide you through deploying a custom Identity Provider (IdP) solution with Okta, allowing organizations to use Okta authentication for their Transfer Family servers. You can leverage built-in features of the Custom IdP solution, such as per-user permissions.\nSolution Architecture The proposed architecture provides a reusable foundation and demonstrates best practices for deploying a custom IdP while allowing granular control over session configuration on a per-user basis, as illustrated in the following figure. This modular architecture promotes code reusability, ease of maintenance, and scalability. This makes it easy to adapt to changing requirements or integrate with other systems.\nBy default, this architecture provides the following features:\nA standard schema in Amazon DynamoDB to store metadata about identity providers, related settings, user credentials, and user session configurations such as HomeDirectoryDetails, Role, and Policy. Support for multiple IdPs connecting to a single Transfer Family server. Support for multiple Transfer Family servers using the same deployment of this solution. Support for the following identity providers: Okta, LDAP (including Microsoft Active Directory), Entra ID, Argon2, Cognito, Public and Private Key, and AWS Secrets Manager. Per-user IP allow-listing. Standardized best-practice logging patterns, with configurable log levels and tracing support. Capability to deploy Amazon API Gateway for advanced use cases. Figure 1: Custom Identity Provider Solution Architecture\nIn this example, we will walk through the steps to deploy and configure Okta as a custom IdP for AWS Transfer Family.\nPrerequisites The following prerequisites are required to deploy this solution:\nAn AWS account with access to AWS CloudShell. Appropriate permissions in AWS Identity and Access Management (IAM) to provision the following resource types: Create IAM roles Create DynamoDB tables Create AWS Lambda functions Create API Gateway (if enabled) An Okta organization. Deployment Guide In this solution, we illustrate a use case where an organization requires Okta authentication with TOTP (time-based one-time password) based MFA for their Transfer Family server. The organization needs the flexibility to assign each user a unique Transfer Family session configuration, such as Role, HomeDirectoryDetails, and PosixProfile.\nIn this walkthrough, we will cover the following steps:\nConfigure Okta: Create a new user and group in Okta Configure MFA Deploy the toolkit: Set up the custom IdP Provision all necessary resources for this solution Define the IdP and user: Configure IdP settings Manage user records in the IdP Test the IdP: Verify the functionality of the deployed solution 1. Configure Okta In this section, you will create a new user and group in your Okta organization and configure MFA.\nFigure 2: Okta Admin Console for Okta Identity Engine\nNote: These instructions apply specifically to Okta Identity Engine. If you are using Okta Classic, the interface and some settings will differ.\nLog in to your Okta organization as an administrator using the admin console (e.g., dev-xxxx-admin.okta.com/admin).\nFigure 3: Adding a user in Okta Admin Console\nNavigate to Directory in the sidebar, select People, and create an Okta user with the username john@demosftp.com. Set a desired password for this user.\nIn the same Directory tab, select Groups, click Add group, and create an Okta group named sftp.\nSelect the newly created group, then choose Assign people and add john@demosftp.com by selecting the plus sign (+) next to their name.\nFigure 4: Adding user to sftp group in Okta Admin Console\nNavigate to Security in the sidebar and select Authenticators. In the Setup tab, choose Add authenticator and add the TOTP authenticator used by your organization. Note: Only TOTP-based authenticators are supported with Transfer Family; push-based authenticators are not currently supported. In this case, you add Google Authenticator as an authenticator.\nTo enforce Multi-Factor Authentication (MFA) with Google Authenticator for SFTP users, navigate to the Enrollment tab and select Add a policy. Enter the following information:\nPolicy name: SFTP Users MFA Assign to groups: SFTP Authenticators: Switch Google Authenticator to Required. Select Create policy. In the appearing Add Rule dialog, set the Rule name to SFTP Users MFA Enrollment and select Create rule. To enforce MFA for user SFTP sessions, navigate to Security in the sidebar and go to Global Session Policy. Select Add policy and set the Policy name to sftp policy. In the Assign to groups section, add sftp, then select Create policy and Add rule.\nIn the appearing Add Rule dialog, set the Rule name to sftp rule. Set Multifactor authentication (MFA) to Required, then select Create rule.\nTo complete the Google Authenticator setup for the Okta user, log in to the Okta domain as john@demosftp.com. Open a private browsing window and navigate to your Okta organization URL (e.g., dev-xxxx.okta.com). At the login screen, enter john@demosftp.com and the password created in Step 1.2. When prompted, select Set up under Google Authenticator and follow the instructions to complete the setup. You only need to configure the Google Authenticator method. Once done, you can skip other MFA methods and select Continue to proceed with login.\nFigure 5: Setting up MFA for an Okta user\nNote: You have successfully added an Okta user to the newly created group and set up MFA for authentication. Before proceeding with the solution, the newly created Okta user must log in to the Okta domain to authenticate and set a new password.\n2. Deploy the Toolkit Log in to the AWS account where you want to deploy the solution, switch to the desired AWS Region to run Transfer Family, and launch a CloudShell session.\nRun the following command to clone the solution repository into your CloudShell environment:\ncd ~ git clone https://github.com/aws-samples/toolkit-for-aws-transfer-family.git Run the following command to execute the build script, which downloads all dependencies and creates archives for the Lambda layer and function used in the solution:\ncd ~/toolkit-for-aws-transfer-family/solutions/custom-idp ./build.sh Monitor the execution process and verify that the script completes successfully.\nDeploy the stack using SAM:\nsam deploy --guided --template \\ ./examples/okta/transfer_okta_customIdp_server_template.yaml --\\ capabilities CAPABILITY_IAM CAPABILITY_AUTO_EXPAND\\ CAPABILITY_NAMED_IAM --disable-rollback When prompted, provide the following information as an example:\nStack Name: transferidp Parameter UserNameDelimiter: @@ Parameter LogLevel: INFO Parameter ProvisionApi: false Parameter EnableTracing: false Parameter UsersTableName: empty Parameter IdentityProvidersTableName: empty For other options, enter Y to keep the default values. Figure 7: Deploying AWS CloudFormation for custom IdP\nWhen the deployment completes successfully, copy/paste the AWS CloudFormation outputs from the deployed stack to a text editor and save them for later use.\nThe output information includes:\nDDBIdentityProvidersTableName: DynamoDB Identity Providers table name DDBUsersTableName: DynamoDB Users table name SFTPServerS3Bucket: S3 bucket name created as backend storage for the Transfer Family server TransferUserRole: Amazon Resource Name (ARN) of the IAM Role created to map to users TransferServerEndpoint: Endpoint of the Transfer Family server 3. Define the IdP Once the Transfer Family custom IdP solution is deployed, you need to define one or more IdPs in the DynamoDB identity_providers table. To use Okta as an IdP, you need to add Okta attributes to this table.\nTable name Comment [StackName]-CustomIdP-XXXX_identity_providers DynamoDB table containing details of each Transfer Family custom IdP (e.g., IdP name, server URL, parameters). [StackName]-CustomIdP-XXXX_users DynamoDB table containing details of each Transfer Family user (e.g., IdP used, IAM role, logical directory list). In the AWS Management Console, navigate to DynamoDB Item Explorer and select the identity_providers table from the table selection menu. Select Create item.\nFigure 8: DynamoDB table for custom IdP\nIn the Create item screen, select JSON view, then copy and paste the following record:\n{ \u0026#34;provider\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;okta\u0026#34; }, \u0026#34;config\u0026#34;: { \u0026#34;M\u0026#34;: { \u0026#34;mfa\u0026#34;: { \u0026#34;BOOL\u0026#34;: true }, \u0026#34;okta_domain\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;[OKTA DOMAIN]\u0026#34; } } }, \u0026#34;module\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;okta\u0026#34; } } Select the Form button to switch back to form view. Replace the placeholder value in the okta_domain field with your Okta organization domain (e.g., dev-xxxxx.okta.com).\nFigure 9: Creating IdP record in DynamoDB\nprovider: IdP name (e.g., okta). module: IdP module name to use (okta). config: Contains configuration details. mfa set to true means users must enter password + TOTP code. Select Create item.\n4. Define a User Next, you need to define a user and map that username to the newly created Okta IdP.\nIn the console, navigate to DynamoDB Item Explorer, select the users table (e.g., [StackName]-CustomIdP-XXXX_users). Select the Create item button.\nFigure 10: DynamoDB table for users\nIn the Create item screen, select JSON view, then copy and paste the following record:\n{ \u0026#34;user\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;john@demosftp.com\u0026#34; }, \u0026#34;identity_provider_key\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;okta\u0026#34; }, \u0026#34;config\u0026#34;: { \u0026#34;M\u0026#34;: { \u0026#34;HomeDirectoryDetails\u0026#34;: { \u0026#34;L\u0026#34;: [ { \u0026#34;M\u0026#34;: { \u0026#34;Entry\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;/\u0026#34; }, \u0026#34;Target\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;/[SFTPServerS3Bucket]/${transfer:UserName}\u0026#34; } } } ] }, \u0026#34;HomeDirectoryType\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;LOGICAL\u0026#34; }, \u0026#34;Role\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;[TransferUserRole]\u0026#34; } } } } Select the Form button. Expand the config section and replace [SFTPServerS3Bucket] and [TransferUserRole] with the corresponding CloudFormation output values from Step 2.5.\nFigure 11: Creating user record in DynamoDB\nSelect Create item.\n5. Test the IdP Navigate to the AWS Transfer Family console and select the deployed server.\nIn the top right corner, select Actions, then select Test from the dropdown menu.\nFigure 12: Testing SFTP access\nEnter john@demosftp.com in the username field. For the password field, enter your password concatenated with the TOTP code from Google Authenticator (Example: if your password is TestP@55w0rd and the MFA code is 201483, enter TestP@55w0rd201483). Source IP can be any value. Select Test.\nFigure 13: Testing authentication with MFA code\nIf successful, an HTTP 200 response will be returned. You can then connect to the Transfer Family server via an SFTP client using the TransferServerEndpoint from the CloudFormation output.\nFigure 14: Testing with SFTP command (note using \u0026ldquo;user@domain\u0026rdquo; format in quotes if needed)\nCleanup To delete the created resources:\nRun the command: sam delete --stack-name \u0026lt;stack-name\u0026gt; Manually delete the two DynamoDB tables (identity_providers and users) in the console. Manually delete the AWS Lambda Layer (handler-layer) in the console. Conclusion Standardizing deployment processes across multiple IdPs improves interoperability and enhances management efficiency. In this post, we guided you on how to deploy Okta as a custom IdP for AWS Transfer Family, integrating MFA for enhanced security. You can explore more about the solution and other modules (such as Active Directory, Entra ID, Cognito) in the GitHub toolkit repository.\nTAGS: AWS Storage Blog, AWS Transfer Family\nAbout the Authors:\nYongki Kim Yongki is an APJ Storage Specialist Solutions Architect with deep expertise in the comprehensive AWS storage portfolio. He is passionate about working with customers to solve architectural challenges.\nTamelly Lim Tamelly is a Specialist Solutions Architect based in Singapore, helping customers design and implement AWS-based solutions. She began her cloud journey in 2022.\n"},{"uri":"https://dokang307.github.io/Internship_Report/5-workshop/5.4-s3-onprem/5.4.1-prepare/","title":"Create Lambda Functions","tags":[],"description":"","content":"Step 1: Create IAM Role for Lambda Go to IAM Console ‚Üí Roles ‚Üí Create role\nTrusted entity type:\nAWS service Use case: Lambda Add permissions:\nAWSLambdaVPCAccessExecutionRole AWSLambdaBasicExecutionRole Role details:\nRole name: daivietblood-lambda-role Description: IAM role for DaiVietBlood Lambda functions Click Create role\nStep 2: Create Lambda Layer for Dependencies Create a folder for dependencies: mkdir -p nodejs cd nodejs npm init -y npm install mysql2 Create zip file: cd .. zip -r mysql2-layer.zip nodejs Go to Lambda Console ‚Üí Layers ‚Üí Create layer\nConfigure:\nName: mysql2-layer Upload: Select mysql2-layer.zip Compatible runtimes: Node.js 18.x, Node.js 20.x Click Create\nStep 3: Create Lambda Function - Get Users Go to Lambda Console ‚Üí Functions ‚Üí Create function\nBasic information:\nFunction name: daivietblood-get-users Runtime: Node.js 20.x Architecture: x86_64 Execution role: Use existing role ‚Üí daivietblood-lambda-role Click Create function\nAdd Layer:\nScroll to Layers ‚Üí Add a layer Custom layers ‚Üí Select mysql2-layer Click Add Configure VPC:\nGo to Configuration ‚Üí VPC ‚Üí Edit VPC: daivietblood-vpc Subnets: Select both Private Subnets Security groups: daivietblood-lambda-sg Click Save Add Environment Variables:\nGo to Configuration ‚Üí Environment variables ‚Üí Edit Add: DB_HOST = daivietblood-db.xxxx.ap-southeast-1.rds.amazonaws.com DB_PORT = 3306 DB_NAME = daivietblood DB_USER = admin DB_PASSWORD = YourSecurePassword123! Click Save Add code in Code tab:\nconst mysql = require(\u0026#39;mysql2/promise\u0026#39;); let connection; const getConnection = async () =\u0026gt; { if (!connection) { connection = await mysql.createConnection({ host: process.env.DB_HOST, port: process.env.DB_PORT, user: process.env.DB_USER, password: process.env.DB_PASSWORD, database: process.env.DB_NAME }); } return connection; }; exports.handler = async (event) =\u0026gt; { try { const conn = await getConnection(); const [rows] = await conn.execute(\u0026#39;SELECT * FROM users\u0026#39;); return { statusCode: 200, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, body: JSON.stringify(rows) }; } catch (error) { console.error(\u0026#39;Error:\u0026#39;, error); return { statusCode: 500, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, body: JSON.stringify({ error: \u0026#39;Internal server error\u0026#39; }) }; } }; Click Deploy Step 4: Create Lambda Function - Create User Create new function: daivietblood-create-user Same configuration as above (VPC, Layer, Environment Variables) Add code: const mysql = require(\u0026#39;mysql2/promise\u0026#39;); let connection; const getConnection = async () =\u0026gt; { if (!connection) { connection = await mysql.createConnection({ host: process.env.DB_HOST, port: process.env.DB_PORT, user: process.env.DB_USER, password: process.env.DB_PASSWORD, database: process.env.DB_NAME }); } return connection; }; exports.handler = async (event) =\u0026gt; { try { const body = JSON.parse(event.body); const { email, name, blood_type, phone } = body; if (!email || !name || !blood_type) { return { statusCode: 400, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, body: JSON.stringify({ error: \u0026#39;Missing required fields\u0026#39; }) }; } const conn = await getConnection(); const [result] = await conn.execute( \u0026#39;INSERT INTO users (email, name, blood_type, phone) VALUES (?, ?, ?, ?)\u0026#39;, [email, name, blood_type, phone || null] ); return { statusCode: 201, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, body: JSON.stringify({ id: result.insertId, email, name, blood_type, phone }) }; } catch (error) { console.error(\u0026#39;Error:\u0026#39;, error); if (error.code === \u0026#39;ER_DUP_ENTRY\u0026#39;) { return { statusCode: 409, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, body: JSON.stringify({ error: \u0026#39;Email already exists\u0026#39; }) }; } return { statusCode: 500, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, body: JSON.stringify({ error: \u0026#39;Internal server error\u0026#39; }) }; } }; Step 5: Create Lambda Function - Emergency Requests Create function: daivietblood-emergency-requests Same configuration Add code: const mysql = require(\u0026#39;mysql2/promise\u0026#39;); let connection; const getConnection = async () =\u0026gt; { if (!connection) { connection = await mysql.createConnection({ host: process.env.DB_HOST, port: process.env.DB_PORT, user: process.env.DB_USER, password: process.env.DB_PASSWORD, database: process.env.DB_NAME }); } return connection; }; exports.handler = async (event) =\u0026gt; { const conn = await getConnection(); const method = event.httpMethod; try { if (method === \u0026#39;GET\u0026#39;) { const [rows] = await conn.execute( \u0026#39;SELECT * FROM emergency_requests WHERE status = \u0026#34;open\u0026#34; ORDER BY urgency DESC, created_at DESC\u0026#39; ); return { statusCode: 200, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, body: JSON.stringify(rows) }; } if (method === \u0026#39;POST\u0026#39;) { const body = JSON.parse(event.body); const { requester_name, blood_type, units_needed, hospital, urgency } = body; const [result] = await conn.execute( \u0026#39;INSERT INTO emergency_requests (requester_name, blood_type, units_needed, hospital, urgency) VALUES (?, ?, ?, ?, ?)\u0026#39;, [requester_name, blood_type, units_needed, hospital, urgency || \u0026#39;normal\u0026#39;] ); return { statusCode: 201, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, body: JSON.stringify({ id: result.insertId, message: \u0026#39;Emergency request created\u0026#39; }) }; } return { statusCode: 405, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, body: JSON.stringify({ error: \u0026#39;Method not allowed\u0026#39; }) }; } catch (error) { console.error(\u0026#39;Error:\u0026#39;, error); return { statusCode: 500, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, body: JSON.stringify({ error: \u0026#39;Internal server error\u0026#39; }) }; } }; Verification Checklist IAM Role created with VPC and Basic execution permissions Lambda Layer created with mysql2 package Lambda functions created and deployed: daivietblood-get-users daivietblood-create-user daivietblood-emergency-requests All functions configured with VPC (Private Subnets) Environment variables set correctly Functions deployed successfully "},{"uri":"https://dokang307.github.io/Internship_Report/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/","title":"Create VPC","tags":[],"description":"","content":"Step 1: Create VPC Go to VPC Console ‚Üí Your VPCs ‚Üí Create VPC\nConfigure VPC:\nResources to create: VPC and more Name tag auto-generation: daivietblood IPv4 CIDR block: 10.0.0.0/16 IPv6 CIDR block: No IPv6 CIDR block Tenancy: Default Configure Subnets:\nNumber of Availability Zones: 2 Number of public subnets: 2 Number of private subnets: 2 Customize subnets CIDR blocks: Public subnet CIDR block in ap-southeast-1a: 10.0.1.0/24 Public subnet CIDR block in ap-southeast-1b: 10.0.2.0/24 Private subnet CIDR block in ap-southeast-1a: 10.0.3.0/24 Private subnet CIDR block in ap-southeast-1b: 10.0.4.0/24 Configure NAT Gateway:\nNAT gateways: In 1 AZ Configure VPC Endpoints:\nVPC endpoints: None (we\u0026rsquo;ll create later if needed) Click Create VPC\n‚ÑπÔ∏è VPC creation takes 2-3 minutes. Wait until status shows \u0026ldquo;Available\u0026rdquo;.\nStep 2: Verify VPC Resources After creation, verify the following resources were created:\nResource Name Details VPC daivietblood-vpc 10.0.0.0/16 Public Subnet 1 daivietblood-subnet-public1-ap-southeast-1a 10.0.1.0/24 Public Subnet 2 daivietblood-subnet-public2-ap-southeast-1b 10.0.2.0/24 Private Subnet 1 daivietblood-subnet-private1-ap-southeast-1a 10.0.3.0/24 Private Subnet 2 daivietblood-subnet-private2-ap-southeast-1b 10.0.4.0/24 Internet Gateway daivietblood-igw Attached to VPC NAT Gateway daivietblood-nat-public1-ap-southeast-1a In Public Subnet 1 Route Table (Public) daivietblood-rtb-public Routes to IGW Route Table (Private) daivietblood-rtb-private1-ap-southeast-1a Routes to NAT Step 3: Create Security Groups 3.1. Security Group for Lambda\nGo to VPC Console ‚Üí Security Groups ‚Üí Create security group\nConfigure:\nSecurity group name: daivietblood-lambda-sg Description: Security group for Lambda functions VPC: Select daivietblood-vpc Inbound rules: (Leave empty - Lambda initiates connections)\nOutbound rules:\nType Protocol Port Destination Description All traffic All All 0.0.0.0/0 Allow all outbound Click Create security group\n3.2. Security Group for RDS\nGo to VPC Console ‚Üí Security Groups ‚Üí Create security group\nConfigure:\nSecurity group name: daivietblood-rds-sg Description: Security group for RDS MySQL VPC: Select daivietblood-vpc Inbound rules:\nType Protocol Port Source Description MySQL/Aurora TCP 3306 daivietblood-lambda-sg Allow Lambda access Outbound rules:\nType Protocol Port Destination Description All traffic All All 0.0.0.0/0 Allow all outbound Click Create security group\n‚ö†Ô∏è Security Best Practice: Only allow access from Lambda Security Group to RDS. Never open port 3306 to 0.0.0.0/0.\nStep 4: Create DB Subnet Group Go to RDS Console ‚Üí Subnet groups ‚Üí Create DB subnet group\nConfigure:\nName: daivietblood-db-subnet-group Description: Subnet group for DaiVietBlood RDS VPC: Select daivietblood-vpc Add subnets:\nAvailability Zones: Select ap-southeast-1a and ap-southeast-1b Subnets: Select both Private Subnets (10.0.3.0/24 and 10.0.4.0/24) Click Create\nVerification Checklist VPC created with CIDR 10.0.0.0/16 2 Public Subnets created 2 Private Subnets created Internet Gateway attached to VPC NAT Gateway created in Public Subnet Route tables configured correctly Lambda Security Group created RDS Security Group created with inbound rule from Lambda SG DB Subnet Group created with Private Subnets "},{"uri":"https://dokang307.github.io/Internship_Report/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nData Resiliency in a Cloud-first World Event Objectives Share best practices in modern application design. Introduce DDD methods and event-driven architecture. Guide on selecting appropriate compute services. Introduce AI tools supporting the development lifecycle. List of Speakers Paul Haverfield - Principal Storage Specialist BDM, APJ Tamelly Lim - Specialist Solutions Architect Ameen Khan S - GTM specialist for Storage - Data \u0026amp; AI pillar covering ASEAN markets Paul Hidalgo - Key Highlights Today, I had the opportunity to attend an AWS program covering an extremely urgent topic in the current context: Data Resiliency. More than just simple backups, the event opened up new perspectives on protecting digital assets against increasingly sophisticated threats.\nHere are the key takeaways I gathered:\n1. Redefining: How is Data Resiliency different from High Availability (HA) and Disaster Recovery (DR)? Previously, we often focused on HA (ensuring systems are always online) or DR (recovering from physical disasters). However, Data Resiliency is a broader and more \u0026ldquo;proactive\u0026rdquo; concept:\nContext: \u0026ldquo;Everything fails, all the time\u0026rdquo; (Werner Vogels) ‚Äì Everything is prone to failure, including physical keys or hardware. The Difference: While HA handles infrastructure incidents, Data Resiliency focuses on data integrity. It is the organizational ability to maintain operations, withstand, and recover even when under cyberattack (such as Ransomware) or human error. Goal: To detect anomalies and automate response processes without human intervention. 2. Why has Data Resiliency become an \u0026ldquo;Absolute Necessity\u0026rdquo;? The explosion of data creation comes with new technological vulnerabilities. Three main trends are driving the shift from Protection to Resiliency:\nRegulatory: Compliance with strict data protection laws. Technology: The complexity of Multi-cloud and Hybrid-cloud environments. Threat Landscape: Ransomware no longer just encrypts primary data; it also targets backups. 3. Data Immutability: The Impenetrable Shield A keyword mentioned repeatedly was Data Immutability.\nThis refers to the ability to create data copies that cannot be changed or deleted for a set period. In the event of a Ransomware attack, even if hackers possess top-level admin rights, they cannot alter this backup. It acts as the \u0026ldquo;Last line of defense,\u0026rdquo; ensuring that at least one clean version always exists for recovery. 4. Protection Strategy: The AWS 3-2-1-1-0 Model The traditional 3-2-1 backup rule has been upgraded to suit the cloud era:\n3 copies of data. 2 different storage media. 1 off-site copy (different region). 1 offline or Immutable (Air-gapped) copy. 0 errors during recovery (verified by automated testing). Important concepts to remember:\nRPO (Recovery Point Objective): How much data loss is acceptable? RTO (Recovery Time Objective): How long does it take to get the system running again? Backup Vault: A container for storing backups, encrypted by AWS KMS for enhanced security. 5. Tool Ecosystem \u0026amp; Solutions The event introduced powerful integrated solutions on AWS:\nCommvault Cloud on AWS: Provides Air Gap Protect (secure data isolation). Cloud Rewind: The ability to \u0026ldquo;rewind\u0026rdquo; time to restore entire instances or VPCs as if the incident never happened. Clumio: An all-in-one simplified backup solution. Uses Serverless Workflow architecture (an army of Lambda functions) to optimize costs and operations. Elastio: Focuses on: Detect, Respond, Recover. Scans for Malware/Ransomware directly within Snapshots to ensure backups do not contain latent malicious code. 6. Workshop Architecture: Real-world Implementation During the hands-on session, we deployed a comprehensive protection model:\nKey Components:\nSource: EC2 Instances (EBS) and S3 Buckets containing critical data. Mechanism: Using AWS Backup Plan with an hourly schedule. Protection Layers: Primary: Stored in a standard Vault (workshop-sources-regular-vault). Secondary (Air-gapped): Copied to another Region (us-east-1-LAG-Vault) with Immutability settings enabled. Validation: Integrated Elastio with AWS Backup. Automated Malware scanning on backups. Performed hourly Restore Testing to ensure backup viability (the \u0026ldquo;0 error\u0026rdquo; strategy). Conclusion The event shifted my mindset from merely \u0026ldquo;backing up data\u0026rdquo; to \u0026ldquo;building resiliency.\u0026rdquo; In an era where cyberattacks are inevitable, possessing a Data Resiliency strategy featuring Immutability and Automation (using tools like Elastio or Commvault) is vital for business survival.\nSome photos from the event Add your photos here Overall, the event not only provided technical knowledge but also helped me change my thinking regarding application design, system modernization, and more effective team collaboration.\n"},{"uri":"https://dokang307.github.io/Internship_Report/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nData Science on AWS Event Objectives Share essential services for data processing (sentiment analysis, comment classification, etc.). List of Speakers Van Hoang Kha - Cloud Solutions Architect, AWS Community Builder Bach Doan Vuong - Cloud DevOps Engineer, AWS Community Builder Key Highlights Below is a summary of the event content focusing on the listed services, presented in a professional report style without icons as requested.\nAWS TRAINING PROGRAM SUMMARY REPORT: AI \u0026amp; MACHINE LEARNING\n1. Overview of Technology Concepts\nTo begin the program, we systematized important foundational concepts in the field of intelligent technology:\nAI (Artificial Intelligence): An overarching concept regarding the creation of intelligent systems. ML (Machine Learning): A subset of AI that allows computers to learn from data. DL (Deep Learning): Uses complex neural networks to model patterns in data. GenAI (Generative AI): Focuses on creating new content and data. 2. AWS as a Service Provider\nThe next section introduced AWS as a comprehensive service provider. AWS offers Managed Services that help businesses apply AI quickly without investing heavily in building infrastructure from scratch.\n3. Details of Introduced AWS Services\nThe training focused deeply on specific tools designed to solve real-world business problems:\nAmazon Comprehend (Natural Language Processing Service - NLP) This service was discussed in the most detail, featuring powerful multi-language text processing capabilities:\nSentiment Analysis: Automatically classifies customer reviews and comments based on positive, negative, or neutral nuances. Text Summarization: Condenses content from long documents. Large-scale Information Processing: Supports bulk email processing and classification. Information Security: Capable of identifying, classifying, and protecting sensitive Personally Identifiable Information (PII) within text. Other Language and Text Processing Services\nAmazon Translate: Automated language translation service. Amazon Textract: A tool for extracting data from scanned documents and papers, including handwriting and complex forms. Amazon Transcribe: A service for converting speech (audio) into written text. Image and Computer Vision Services\nAmazon Rekognition: A Deep Learning-based service specialized for analyzing images and videos (object detection, facial recognition, content moderation). Customer Experience Services\nAmazon Personalize: A solution to enhance customer experience through personalization. This service records and analyzes user behavior, thereby providing product or content recommendations best suited to individual preferences. Technical Infrastructure\nSageMaker Instance: Provides the server environment and tools necessary for developers to self-build, train, and deploy custom machine learning models according to specific needs. Some photos from the event Add your photos here Overall, the event not only provided technical knowledge but also helped me change my thinking regarding application design, system modernization, and more effective team collaboration.\n"},{"uri":"https://dokang307.github.io/Internship_Report/4-eventparticipated/4.4-event4/","title":"Event 4","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only; please do not copy it verbatim for your report, including this warning.\nSummary Report: Attending AWS Cloud Mastery Series #1 - AI/ML/GenAI on AWS\nAttendance Objectives Recently, I had the opportunity to attend the opening event for the \u0026ldquo;AWS Cloud Mastery\u0026rdquo; series, focusing on AI, Machine Learning, and Generative AI. My main goal was to update my comprehensive view of these technologies on the AWS platform and learn how to apply them to real-world business problems.\nSpeakers The session featured sharing from experienced experts in the industry, including Mr. Lam Tuan Kiet (Sr DevOps Engineer - FPT Software), Mr. Danh Hoang Hieu Nghi (AI Engineer - Renova Cloud), Mr. Dinh Le Hoang Anh (Cloud Engineer Trainee), and Mr. Van Hoang Kha (Community Builder).\nValuable Knowledge I Harvested:\n1. The Power of Generative AI on Amazon Bedrock This was the part that impressed me the most. Amazon Bedrock acts as a central platform, providing access to leading Foundation Models (FMs) from Anthropic, OpenAI, Meta, etc. This allows us to fine-tune existing models without having to build a model from scratch.\nI also reinforced my skills in Prompt Engineering, understanding better how to guide the model through various strategies:\nZero-shot: Providing a request directly without examples. Few-shot: Providing a handful of examples for the model to mimic. Chain-of-Thought: Asking the model to explain its reasoning steps for a more logical result. Specifically, the RAG (Retrieval Augmented Generation) technique was highlighted as an optimal solution to improve accuracy:\nRetrieval: Pulling real data from the enterprise knowledge base. Augmentation: Adding that data as context for the prompt. Generation: The model answers based on factual information, minimizing hallucinations. Additionally, Amazon Titan Embeddings was introduced as a tool to convert text into vectors, serving semantic search and multilingual RAG workflows.\n2. AWS AI Services Ecosystem Beyond GenAI, I also reviewed AWS\u0026rsquo;s \u0026ldquo;ready-made\u0026rdquo; AI services (APIs) that help solve specific problems quickly without complex model training:\nImage/Video Analysis (Rekognition). Translation (Translate) and Speech-to-Text/Text-to-Speech (Transcribe, Polly). Data Extraction (Textract) and Natural Language Processing (Comprehend). Intelligent Search (Kendra) or Anomaly Detection (Lookout). The AMZPhoto face recognition demo visually illustrated how to integrate these services into a real product.\n3. Amazon Bedrock AgentCore ‚Äì Putting AI Agents into Practice This is a new framework helping to solve the problem of operating AI Agents at scale (production-ready). It supports long-term memory management, identity security, tool integration (like browsers, code interpreters), and most importantly, observability. This makes deploying frameworks like CrewAI or LangGraph safer and more effective on the AWS platform.\nPlan for Application at Work Based on what I learned, I plan to apply the following knowledge immediately:\nDeploy RAG \u0026amp; AgentCore: Propose and apply these to upcoming internal projects requiring GenAI features to increase accuracy and automation capabilities. Optimize Development Process: Use available AWS AI Services instead of building from scratch to shorten Time-to-market. Improve Model Performance: Apply advanced Prompt Engineering techniques to optimize output results for current AI tasks. Side Experience Not only did I absorb knowledge, but the event atmosphere was also extremely lively. I was lucky enough to reach the Top 6 in the Kahoot competition; however, I dropped off the leaderboard during the final questions. Nevertheless, it was still an interesting game that helped me reinforce the knowledge from this event.\n"},{"uri":"https://dokang307.github.io/Internship_Report/4-eventparticipated/4.5-event5/","title":"Event 5","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only; please do not copy it verbatim for your report, including this warning.\nSummary Report: Attending AWS Cloud Mastery Series #2 ‚Äì DevOps on AWS Attendance Objectives Continuing the event series, I attended the second session focusing on DevOps. My goal was to master AWS services supporting DevOps, deepen my understanding of CI/CD pipeline design, Infrastructure as Code (IaC) concepts, as well as how to deploy and monitor containerized applications on the AWS platform.\nSpeakers The session gathered a strong lineup of speakers, including AWS Community Builders and experienced engineers:\nMr. Truong Quang Tinh ‚Äì Platform Engineer (TymeX) Mr. Bao Huynh, Nguyen Khanh Phuc Thinh, Tran Dai Vi, Huynh Hoang Long, Pham Hoang Quy, Nghiem Le (AWS Community Builders) Mr. Dinh Le Hoang Anh ‚Äì Cloud Engineer Trainee (First Cloud AI Journey) Valuable Knowledge I Harvested: 1. Building a DevOps Foundation The speakers helped me redefine that DevOps is not a job title but a mindset and habit of working. The core lies in:\nAutomating repetitive tasks. Sharing knowledge across teams. Continuously experimenting and learning. Measuring effectiveness with real data rather than assumptions. I also learned lessons about common pitfalls for beginners: avoid getting stuck in \u0026ldquo;tutorial hell\u0026rdquo; without starting real projects, and focus on personal progress rather than comparing oneself to others.\n2. Infrastructure as Code (IaC) in Practice This section broadened my horizon regarding IaC tools instead of sticking to a single one. The speakers provided a detailed comparison:\nCloudFormation: AWS\u0026rsquo;s native template tool. AWS CDK: For developers who prefer writing infrastructure using familiar programming languages. Terraform: The optimal choice for teams working on multi-cloud platforms. The most important message: Infrastructure built via Code (IaC) is significantly more consistent, maintainable, and secure compared to manual configuration (ClickOps).\n3. Containers and Deployment Models The content ranged from basics (Dockerfile, Image) to advanced AWS services:\nAmazon ECR: Storage and security scanning for images. Amazon ECS \u0026amp; EKS: Two popular container orchestration options. The comparison between ECS (simple, native) and EKS (powerful with Kubernetes) helped me know when to use which. AWS App Runner: A quick deployment solution without worrying about cluster management. 4. Monitoring and Observability A system cannot lack monitoring capabilities. I understood better the roles of:\nAmazon CloudWatch: The center for metrics, logs, and alarms. AWS X-Ray: A tracing tool helping visualize request flows and detect bottlenecks. Core lesson: Observability features must be designed from the very beginning, not added after the system is built.\nPlan for Application at Work Specifically, I plan to apply this knowledge to the team\u0026rsquo;s upcoming AI Chatbot Project:\nEstablish CI/CD Pipeline: Use AWS CodePipeline to automate the entire process from Build, Test, to Deploy. The goal is to ensure every code update is tested and pushed to production smoothly. Implement IaC: Use AWS CDK to define all resources (Lambda, API Gateway, DynamoDB, S3, IAM\u0026hellip;). This makes the system easy to reuse for different environments and scale quickly when needed. Applying this process will help the Chatbot project develop faster, minimize human error, and make operations easier.\nEvent Experience The session gave me a practical perspective on how modern businesses implement DevOps on AWS. Beyond theory, the real-world examples from speakers about CloudFormation, Terraform, or how to choose EKS/ECS were truly valuable.\nBeyond professional knowledge, this was also a good occasion for me to network with like-minded friends and learn \u0026ldquo;hard-earned\u0026rdquo; experiences from those who came before me.\n"},{"uri":"https://dokang307.github.io/Internship_Report/4-eventparticipated/4.6-event6/","title":"Event 6","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy it verbatim for your report, including this warning.\nEvent Summary Report: AWS Cloud Mastery Series #3 Topic: AWS Well-Architected ‚Äì Security Pillar Workshop\n1. Overview \u0026amp; Speakers The event focused on the most critical pillar within the AWS Well-Architected Framework: Security. The content was designed to equip attendees with knowledge ranging from fundamental to advanced levels regarding identity, monitoring, infrastructure protection, data protection, and incident response processes.\nSpeakers \u0026amp; Experts: The event gathered experts from the AWS Community (AWS Community Builders), AWS Cloud Club Captains from various universities (HCMUTE, SGU, PTIT, HUFLIT), Cloud Engineers from FCJ, and specially featuring Mendel Grabski (Security \u0026amp; DevOps Expert).\nAbout AWS Cloud Club: This is a network connecting students and professionals, helping to develop technical leadership skills, providing hands-on experiences, and offering long-term mentoring opportunities. The participating Cloud Clubs under FCJA include: HCMUTE, SGU, PTIT, and HUFLIT.\n2. Key Technical Highlights A. Identity \u0026amp; Access Management (IAM) IAM is defined as the \u0026ldquo;first line of defense.\u0026rdquo; The session emphasized the shift from manual management to automation and strict adherence to key principles:\nLeast Privilege Principle: Grant only the necessary permissions required to perform a task. Root User Protection: Delete access keys immediately after creation. Service Control Policies (SCPs): Use Organization-level policies to set a \u0026ldquo;ceiling\u0026rdquo; (maximum available permissions) for member accounts (Note: SCPs only filter permissions; they do not grant them). Permission Boundaries: Set the maximum permissions that an identity-based policy can grant to a specific User/Role. Multi-Factor Authentication (MFA): Encouraged the use of FIDO2 (hardware keys/biometrics) over traditional TOTP. Credential Rotation: Use AWS Secrets Manager to automate the rotation process (create -\u0026gt; set -\u0026gt; test -\u0026gt; finish) and integrate with EventBridge to manage schedules, eliminating risks associated with \u0026ldquo;hardcoded credentials.\u0026rdquo; B. Continuous Monitoring \u0026amp; Threat Detection The focus was on building comprehensive visibility and automated response capabilities:\nMulti-Layer Monitoring: Combining CloudTrail (recording API calls, S3/Lambda access) and VPC Flow Logs (network traffic). Event-Driven Security: Using EventBridge as a central event bus to route real-time alerts to Lambda/SNS/SQS or coordinate actions across different accounts (Cross-account routing). Detection-as-Code: Managing detection rules and queries (CloudTrail Lake queries) as code (version control) to ensure consistent deployment across the organization. Deep Dive into Amazon GuardDuty: This is an intelligent threat detection solution that operates continuously based on three main data sources: CloudTrail, VPC Flow Logs, and DNS Logs.\nExpanded Coverage: Protection for S3, EKS, RDS (brute-force detection), Lambda (suspicious network activity), and Malware Protection (EBS scanning). Runtime Monitoring: Uses an Agent to monitor deep inside the OS (processes, file access) on EC2/EKS/Fargate. C. Compliance \u0026amp; Infrastructure as Code (IaC) Security compliance is no longer a manual check but is integrated into the deployment pipeline:\nApplied Standards: AWS Foundational Security Best Practices, CIS Benchmark, PCI DSS, NIST. Enforcement Mechanism: Using AWS CloudFormation (IaC) to deploy standard configurations, combined with AWS Security Hub to automatically audit resources against defined standards. D. Network \u0026amp; Data Protection Network Security: Clearly distinguishing between Security Groups (Stateful - instance level firewall) and NACLs (Stateless - subnet level firewall). Introduction to AWS Network Firewall for advanced Egress filtering/IPS and integration with Threat Intelligence to automatically block malicious traffic. Data Protection: Encryption: Using KMS with Customer Master Keys (CMK) and Policy conditions to control decryption contexts. Certificates: Using AWS ACM to manage and automatically renew SSL/TLS certificates. Service Security: Enforcing HTTPS/TLS 1.2+ for S3 (via Bucket Policy) and Databases (e.g., PostgreSQL rds.force_ssl=1). E. Incident Response (IR) The standard IR process consists of 5 steps: Preparation -\u0026gt; Detection \u0026amp; Analysis -\u0026gt; Containment -\u0026gt; Eradication \u0026amp; Recovery -\u0026gt; Post-Incident Activity.\nPrevention Strategy: Never make S3 buckets public, isolate sensitive services in private subnets, and ensure all infrastructure changes go through IaC with a review process (double-gate). 3. Practical Experience \u0026amp; Q\u0026amp;A The event provided high practical value, specifically aligning with the \u0026ldquo;Automated Incident Response and Forensics\u0026rdquo; project our team is developing.\nDiscussion Point: During our project testing, the team noticed that Amazon GuardDuty has a latency of about 5 minutes to generate a finding after an incident occurs. We asked about solutions to reduce this latency.\nExpert Answer:\nThe Nature of the Service: The GuardDuty latency is an accepted technical characteristic because the system needs time to analyze large datasets to accurately determine threats and avoid false positives. Alternative Solutions: To achieve near real-time detection, the expert suggested integrating 3rd party solutions like OpenClarity (open source) or building custom anomaly detection logic based on CloudTrail events. Networking: After the event, Mr. Mendel Grabski (Ex-Head of Security \u0026amp; DevOps) expressed interest and offered professional support for our project, opening up valuable collaboration and mentoring opportunities.\nSome photos from the event participation [Add your photos here] Overall, the event not only provided technical knowledge but also changed my mindset regarding application design, system modernization, and effective team collaboration.\n"},{"uri":"https://dokang307.github.io/Internship_Report/","title":"Internship Report","tags":[],"description":"","content":"Internship Report ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nStudent Information: Full Name: Do Khang\nPhone Number: 0943091761\nEmail: dokhang307@gmail.com\nUniversity: FPT University HCM\nMajor: Artificial Inteligent\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 28/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://dokang307.github.io/Internship_Report/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Goals: Environment setup. Understand basic AWS services, how to use the Console \u0026amp; CLI. Tasks to be implemented this week: Day Task Start Date Completion Date Reference 2 - Learn how to draw diagrams using AWS icons\n- Course preparation (install required packages: powershell, winget, go, hugo)\n- Learn how to write markdown and create a website with Hugo 08/09/2025 08/09/2025 3 - Getting started with AWS\n- What is AWS\n- AWS Infrastructure\n- Cost optimization and AWS Support\n- Create an AWS account 09/09/2025 09/09/2025 Creating your first AWS account 4 - Learn about Amazon Virtual Private Cloud (VPC)\n- VPC\n- Subnet\n- Route table\n- Elastic Network Interface (ENI)\n- VPC Endpoint\n- Internet Gateway\n- NAT Gateway 10/09/2025 10/09/2025 https://youtu.be/O5CIvG0Wt78\nhttps://youtu.be/DHYgA94SefE\nhttps://youtu.be/dHoYmQR7FYs 5 - VPC Security and Multi-VPC features\n- Security Group (SG)\n- Network Access Control List (NACL)\n- VPC Flow Logs\n- VPC Peering \u0026amp; Transit Gateway\n- VPC Peering\n- Transit Gateway 11/09/2025 11/09/2025 6 - VPN \u0026amp; Direct Connect\n- VPN Site-to-Site\n- VPN Client-to-Site\n- AWS Direct Connect\n- Elastic Load Balancer\n- Elastic Load Balancing\n- Network Load Balancer\n- Gateway Load Balancer 12/09/2025 12/09/2025 Week 1 achievement: Mastering Networking Fundamentals and Security Mindset Core Knowledge: Established a solid framework for understanding AWS Global Infrastructure and Amazon VPC.\nPractical Skills:\nSetup: Completed secure AWS account creation (utilizing IAM Users) and set up the documentation environment (Hugo/Markdown). VPC: Successfully created a VPC manually, including Subnets (Public/Private), IGW, and a functional NAT Gateway. Network Security: Learned to distinguish between Security Groups and NACLs and how to apply them effectively to control network traffic. Cloud Mindset: Began developing a cloud-native mindset focused on security (Least Privilege) and connection optimization (VPC Endpoints). Challenges: Calculating CIDR Blocks and accurately updating Route Tables during NAT Gateway and VPC Peering configurations required high precision and was initially confusing. Further practice is needed.\n"},{"uri":"https://dokang307.github.io/Internship_Report/1-worklog/","title":"Worklog","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nWeek 1: Getting familiar with AWS and basic AWS services\nWeek 2: Doing task A\u0026hellip;\nWeek 3: Doing task B\u0026hellip;\nWeek 4: Doing task C\u0026hellip;\nWeek 5: Doing task D\u0026hellip;\nWeek 6: Doing task E\u0026hellip;\nWeek 7: Doing task G\u0026hellip;\nWeek 8: Doing task H\u0026hellip;\nWeek 9: Doing task I\u0026hellip;\nWeek 10: Doing task L\u0026hellip;\nWeek 11: Doing task M\u0026hellip;\nWeek 12: Doing task N\u0026hellip;\n"},{"uri":"https://dokang307.github.io/Internship_Report/5-workshop/5.1-workshop-overview/","title":"Workshop Overview","tags":[],"description":"","content":"System Architecture The DaiVietBlood system uses a Serverless-First architecture on AWS Cloud, prioritizing scalability, security, and operational optimization.\nArchitecture Components 1. Network Infrastructure (VPC)\nComponent Description VPC Virtual Private Cloud with CIDR 10.0.0.0/16 Public Subnet Contains NAT Gateway, allows Internet access Private Subnet Contains Lambda, RDS - isolated from Internet NAT Gateway Allows Private Subnet resources to access Internet Internet Gateway Allows Public Subnet to communicate with Internet 2. Application \u0026amp; Data Layer\nService Role AWS Lambda Process business logic (CRUD operations, emergency requests) API Gateway Receive HTTP requests, route to Lambda Amazon RDS MySQL database storing user data, blood inventory Amazon S3 Store static files (images, documents) 3. Frontend \u0026amp; Distribution\nService Role AWS Amplify Host React application CloudFront CDN distributes content globally with low latency 4. DevOps \u0026amp; Monitoring\nService Role CodePipeline Automate CI/CD process CodeBuild Build and test source code CloudWatch Collect logs, metrics, set up alarms Data Flow User Request ‚Üí CloudFront ‚Üí Amplify (Frontend) ‚Üì API Gateway ‚Üì AWS Lambda (Private Subnet) ‚Üì Amazon RDS (Private Subnet) Security Model Network Isolation: RDS and Lambda in Private Subnet, no direct Internet access IAM Roles: Each service has minimum required permissions (Least Privilege) Data Encryption: At-rest (RDS, S3) and In-transit (HTTPS) Security Groups: Control inbound/outbound traffic for each resource Workshop Objectives After completing this workshop, you will be able to:\n‚úÖ Create VPC with proper network segmentation ‚úÖ Deploy RDS MySQL in Private Subnet ‚úÖ Build Lambda functions and expose via API Gateway ‚úÖ Configure S3 and CloudFront for static content ‚úÖ Deploy React app with Amplify ‚úÖ Set up CI/CD pipeline ‚úÖ Monitor with CloudWatch "},{"uri":"https://dokang307.github.io/Internship_Report/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Goals: Tasks to be implemented this week: Day Task Start Date Completion Date Reference 2 - Create AWS account and setup MFA (Multi-Factor Authentication)\n- Create admin group and admin user\n- Verification\n- Create budgets and manage cost budgets\n- Explore AWS Support Plans 15/09/2025 15/09/2025 - MFA for AWS account\n- Creating Admin Group and Admin User 3 Theory Research:\n- VPC (Virtual Private Cloud)\n- Subnet\n- Route Table\n- Internet Gateway\n- Security Group\n- Network Access Control List (Network ACLs)\nPractice:\n- Create VPC, subnet, Internet Gateway, Route Table, Security Group, EC2 Instance, NAT Gateway\n- Check connectivity (failed) ‚Üí Clean up resources 16/09/2025 16/09/2025 - Introduction to Amazon VPC\n- Subnets\n- Route Table\n- Internet Gateway\n- NAT Gateway 4 - Re-practice and check connectivity\n- Learn and practice setting up Hybrid DNS system with Route 53 17/09/2025 17/09/2025 Route 53 5 - Attend AWS Vietnam Cloud Day event 18/09/2025 18/09/2025 6 - Practice setting up VPC Peering\n- Setup Transit Gateway 19/09/2025 19/09/2025 - VPC Peering\n- Transit Gateway Week 2 achievement Account Security \u0026amp; Governance: Successfully implemented critical security steps like MFA and applied IAM best practices (using Admin User instead of Root). Setting up AWS Budgets is a good habit to maintain. VPC Deployment \u0026amp; Troubleshooting: I manually deployed a full VPC architecture (IGW, NAT Gateway) and, most importantly, experienced Troubleshooting. Connection failures (though time-consuming) were the most valuable lessons, helping me clearly understand the role of each component in Route Tables and Security Groups. Advanced Networking: Grasped how to connect extended networks (VPC Peering and Transit Gateway) and started approaching Hybrid Cloud via Route 53 Resolver. Industry Engagement: Attending AWS Vietnam Cloud Day provided real-world context and technology trends, bridging learned knowledge with industry applications. "},{"uri":"https://dokang307.github.io/Internship_Report/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/","title":"Create Amazon RDS","tags":[],"description":"","content":"Step 1: Create RDS MySQL Instance Go to RDS Console ‚Üí Databases ‚Üí Create database\nChoose database creation method:\nStandard create Engine options:\nEngine type: MySQL Engine version: MySQL 8.0.35 (or latest 8.0.x) Templates:\nFree tier (for workshop/testing) Settings:\nDB instance identifier: daivietblood-db Master username: admin Credentials management: Self managed Master password: YourSecurePassword123! Confirm password: YourSecurePassword123! ‚ö†Ô∏è Important: Save your password securely. You will need it to connect from Lambda.\nStep 2: Instance Configuration Instance configuration: DB instance class: db.t3.micro (Free tier eligible) Storage type: General Purpose SSD (gp2) Allocated storage: 20 GiB Storage autoscaling: Disable (for cost control) Step 3: Connectivity Connectivity:\nCompute resource: Don\u0026rsquo;t connect to an EC2 compute resource Network type: IPv4 Virtual private cloud (VPC): daivietblood-vpc DB subnet group: daivietblood-db-subnet-group Public access: No ‚ö†Ô∏è Important! VPC security group: Choose existing Existing VPC security groups: daivietblood-rds-sg Availability Zone: ap-southeast-1a Database port:\nDatabase port: 3306 Step 4: Database Authentication Database authentication: Password authentication Step 5: Additional Configuration Database options:\nInitial database name: daivietblood DB parameter group: default.mysql8.0 Option group: default:mysql-8-0 Backup:\nEnable automated backups: Yes Backup retention period: 7 days Backup window: No preference Encryption:\nEnable encryption: Yes (default) Monitoring:\nEnable Enhanced monitoring: No (to reduce cost) Maintenance:\nEnable auto minor version upgrade: Yes Maintenance window: No preference Deletion protection:\nEnable deletion protection: No (for workshop) Click Create database\n‚ÑπÔ∏è RDS creation takes 10-15 minutes. Wait until status shows \u0026ldquo;Available\u0026rdquo;.\nStep 6: Get RDS Endpoint After RDS is available:\nGo to RDS Console ‚Üí Databases ‚Üí Click daivietblood-db\nIn Connectivity \u0026amp; security tab, copy:\nEndpoint: daivietblood-db.xxxxxxxxxxxx.ap-southeast-1.rds.amazonaws.com Port: 3306 Save these values for Lambda configuration:\nDB_HOST=daivietblood-db.xxxxxxxxxxxx.ap-southeast-1.rds.amazonaws.com DB_PORT=3306 DB_NAME=daivietblood DB_USER=admin DB_PASSWORD=YourSecurePassword123! Step 7: Create Database Schema Since RDS is in Private Subnet, you need to connect via a bastion host or use Lambda to initialize the schema.\nOption A: Using Lambda to Initialize (Recommended)\nCreate a one-time Lambda function to initialize the database:\n// init-db.js const mysql = require(\u0026#39;mysql2/promise\u0026#39;); exports.handler = async (event) =\u0026gt; { const connection = await mysql.createConnection({ host: process.env.DB_HOST, user: process.env.DB_USER, password: process.env.DB_PASSWORD, database: process.env.DB_NAME }); // Create tables const createUsersTable = ` CREATE TABLE IF NOT EXISTS users ( id INT AUTO_INCREMENT PRIMARY KEY, email VARCHAR(255) UNIQUE NOT NULL, name VARCHAR(255) NOT NULL, blood_type ENUM(\u0026#39;A+\u0026#39;, \u0026#39;A-\u0026#39;, \u0026#39;B+\u0026#39;, \u0026#39;B-\u0026#39;, \u0026#39;AB+\u0026#39;, \u0026#39;AB-\u0026#39;, \u0026#39;O+\u0026#39;, \u0026#39;O-\u0026#39;) NOT NULL, phone VARCHAR(20), created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ) `; const createDonationsTable = ` CREATE TABLE IF NOT EXISTS donations ( id INT AUTO_INCREMENT PRIMARY KEY, user_id INT NOT NULL, donation_date DATE NOT NULL, location VARCHAR(255), status ENUM(\u0026#39;scheduled\u0026#39;, \u0026#39;completed\u0026#39;, \u0026#39;cancelled\u0026#39;) DEFAULT \u0026#39;scheduled\u0026#39;, created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP, FOREIGN KEY (user_id) REFERENCES users(id) ) `; const createEmergencyRequestsTable = ` CREATE TABLE IF NOT EXISTS emergency_requests ( id INT AUTO_INCREMENT PRIMARY KEY, requester_name VARCHAR(255) NOT NULL, blood_type ENUM(\u0026#39;A+\u0026#39;, \u0026#39;A-\u0026#39;, \u0026#39;B+\u0026#39;, \u0026#39;B-\u0026#39;, \u0026#39;AB+\u0026#39;, \u0026#39;AB-\u0026#39;, \u0026#39;O+\u0026#39;, \u0026#39;O-\u0026#39;) NOT NULL, units_needed INT NOT NULL, hospital VARCHAR(255) NOT NULL, urgency ENUM(\u0026#39;critical\u0026#39;, \u0026#39;urgent\u0026#39;, \u0026#39;normal\u0026#39;) DEFAULT \u0026#39;normal\u0026#39;, status ENUM(\u0026#39;open\u0026#39;, \u0026#39;fulfilled\u0026#39;, \u0026#39;cancelled\u0026#39;) DEFAULT \u0026#39;open\u0026#39;, created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ) `; await connection.execute(createUsersTable); await connection.execute(createDonationsTable); await connection.execute(createEmergencyRequestsTable); await connection.end(); return { statusCode: 200, body: JSON.stringify({ message: \u0026#39;Database initialized successfully\u0026#39; }) }; }; Verification Checklist RDS instance created and status is \u0026ldquo;Available\u0026rdquo; RDS is in Private Subnet (Public access: No) RDS Security Group only allows access from Lambda SG Endpoint and credentials saved securely Initial database daivietblood created Database schema initialized (tables created) Troubleshooting Issue Solution Cannot connect to RDS Verify Security Group allows inbound from Lambda SG RDS creation failed Check Service Quotas for RDS instances Connection timeout Ensure Lambda is in same VPC and has NAT Gateway access "},{"uri":"https://dokang307.github.io/Internship_Report/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/","title":"Create API Gateway","tags":[],"description":"","content":"Step 1: Create REST API Go to API Gateway Console ‚Üí Create API\nChoose API type:\nREST API ‚Üí Build Create new API:\nProtocol: REST Create new API: New API API name: daivietblood-api Description: REST API for DaiVietBlood system Endpoint Type: Regional Click Create API\nStep 2: Create Resources 2.1. Create /users Resource\nSelect root / ‚Üí Actions ‚Üí Create Resource\nConfigure:\nResource Name: users Resource Path: users Enable API Gateway CORS: ‚úÖ Check Click Create Resource\n2.2. Create /emergency-requests Resource\nSelect root / ‚Üí Actions ‚Üí Create Resource\nConfigure:\nResource Name: emergency-requests Resource Path: emergency-requests Enable API Gateway CORS: ‚úÖ Check Click Create Resource\nStep 3: Create Methods for /users 3.1. GET /users\nSelect /users ‚Üí Actions ‚Üí Create Method ‚Üí GET\nIntegration setup:\nIntegration type: Lambda Function Use Lambda Proxy integration: ‚úÖ Check Lambda Region: ap-southeast-1 Lambda Function: daivietblood-get-users Click Save ‚Üí OK (to add permission)\n3.2. POST /users\nSelect /users ‚Üí Actions ‚Üí Create Method ‚Üí POST\nIntegration setup:\nIntegration type: Lambda Function Use Lambda Proxy integration: ‚úÖ Check Lambda Region: ap-southeast-1 Lambda Function: daivietblood-create-user Click Save ‚Üí OK\nStep 4: Create Methods for /emergency-requests 4.1. GET /emergency-requests\nSelect /emergency-requests ‚Üí Actions ‚Üí Create Method ‚Üí GET\nIntegration setup:\nIntegration type: Lambda Function Use Lambda Proxy integration: ‚úÖ Check Lambda Function: daivietblood-emergency-requests Click Save ‚Üí OK\n4.2. POST /emergency-requests\nSelect /emergency-requests ‚Üí Actions ‚Üí Create Method ‚Üí POST\nIntegration setup:\nIntegration type: Lambda Function Use Lambda Proxy integration: ‚úÖ Check Lambda Function: daivietblood-emergency-requests Click Save ‚Üí OK\nStep 5: Enable CORS For each resource (/users, /emergency-requests):\nSelect resource ‚Üí Actions ‚Üí Enable CORS\nConfigure:\nAccess-Control-Allow-Methods: GET, POST, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Amz-Date, Authorization, X-Api-Key Access-Control-Allow-Origin: * Click Enable CORS and replace existing CORS headers\nClick Yes, replace existing values\nStep 6: Deploy API Actions ‚Üí Deploy API\nDeployment stage:\nDeployment stage: [New Stage] Stage name: prod Stage description: Production stage Click Deploy\nCopy the Invoke URL:\nhttps://xxxxxxxxxx.execute-api.ap-southeast-1.amazonaws.com/prod ‚ÑπÔ∏è Save this URL. You will need it for frontend configuration.\nStep 7: API Structure Summary After completing, your API structure should look like:\ndaivietblood-api ‚îÇ ‚îú‚îÄ‚îÄ /users ‚îÇ ‚îú‚îÄ‚îÄ GET ‚Üí daivietblood-get-users ‚îÇ ‚îú‚îÄ‚îÄ POST ‚Üí daivietblood-create-user ‚îÇ ‚îî‚îÄ‚îÄ OPTIONS (CORS) ‚îÇ ‚îî‚îÄ‚îÄ /emergency-requests ‚îú‚îÄ‚îÄ GET ‚Üí daivietblood-emergency-requests ‚îú‚îÄ‚îÄ POST ‚Üí daivietblood-emergency-requests ‚îî‚îÄ‚îÄ OPTIONS (CORS) Verification Checklist REST API created /users resource created with GET and POST methods /emergency-requests resource created with GET and POST methods CORS enabled for all resources API deployed to prod stage Invoke URL saved "},{"uri":"https://dokang307.github.io/Internship_Report/5-workshop/5.2-prerequiste/","title":"Preparation","tags":[],"description":"","content":"Prerequisites Before starting this workshop, ensure you have:\n1. AWS Account\nActive AWS Account with Administrator access Recommended: Use IAM User instead of Root account Region: Asia Pacific (Singapore) - ap-southeast-1 2. Local Development Tools\nTool Version Purpose Node.js \u0026gt;= 18.x Run Lambda functions locally npm/yarn Latest Package management AWS CLI \u0026gt;= 2.x Interact with AWS services Git Latest Version control 3. Knowledge Requirements\nBasic understanding of AWS services (VPC, EC2, S3) Familiarity with REST APIs Basic Node.js/JavaScript or Python Basic React knowledge Step 1: Configure AWS CLI Install AWS CLI from AWS CLI Installation Guide\nConfigure credentials:\naws configure Enter your credentials: AWS Access Key ID: [Your Access Key] AWS Secret Access Key: [Your Secret Key] Default region name: ap-southeast-1 Default output format: json Verify configuration: aws sts get-caller-identity Step 2: Create IAM User for Workshop Go to IAM Console ‚Üí Users ‚Üí Create user\nUser details:\nUser name: workshop-admin Select: Provide user access to the AWS Management Console Set permissions:\nSelect: Attach policies directly Search and select: AdministratorAccess Create user and save credentials securely\n‚ö†Ô∏è Security Note: After completing the workshop, delete this IAM user or remove AdministratorAccess policy.\nStep 3: Verify Service Quotas Ensure your account has sufficient quotas for:\nService Resource Minimum Required VPC VPCs per Region 1 VPC Subnets per VPC 4 VPC NAT Gateways per AZ 1 RDS DB Instances 1 Lambda Concurrent Executions 10 API Gateway REST APIs 1 S3 Buckets 2 Check quotas at: Service Quotas Console ‚Üí Select service ‚Üí View quotas\nStep 4: Prepare Source Code Clone the sample repository: git clone https://github.com/your-repo/daivietblood-workshop.git cd daivietblood-workshop Project structure: daivietblood-workshop/ ‚îú‚îÄ‚îÄ frontend/ # React application ‚îÇ ‚îú‚îÄ‚îÄ src/ ‚îÇ ‚îî‚îÄ‚îÄ package.json ‚îú‚îÄ‚îÄ backend/ # Lambda functions ‚îÇ ‚îú‚îÄ‚îÄ functions/ ‚îÇ ‚îî‚îÄ‚îÄ package.json ‚îú‚îÄ‚îÄ infrastructure/ # CloudFormation templates ‚îÇ ‚îî‚îÄ‚îÄ templates/ ‚îî‚îÄ‚îÄ README.md Install dependencies: # Frontend cd frontend \u0026amp;\u0026amp; npm install # Backend cd ../backend \u0026amp;\u0026amp; npm install Step 5: Cost Estimation Service Configuration Est. Cost/Day NAT Gateway 1 NAT Gateway ~$1.08 RDS db.t3.micro ~$0.52 Lambda Free Tier $0.00 API Gateway Free Tier $0.00 S3 \u0026lt; 5GB ~$0.01 CloudFront \u0026lt; 1GB transfer ~$0.01 Amplify Build \u0026amp; Host ~$0.50 Total estimated: ~$2-3/day\nüí° Tip: Complete the workshop in 1-2 days and clean up resources immediately to minimize costs.\nChecklist Before Starting AWS Account ready with Administrator access AWS CLI installed and configured Node.js \u0026gt;= 18.x installed Git installed Source code cloned Region set to ap-southeast-1 "},{"uri":"https://dokang307.github.io/Internship_Report/2-proposal/","title":"Proposal","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nDai Viet Blood Donation \u0026amp; Emergency System (DaiVietBlood) Implemented by: Skyline Team ‚Äì FPT University Ho Chi Minh City\nDate: December 7, 2025\nDownload PDF\nTABLE OF CONTENTS BACKGROUND AND MOTIVATION 1.1 Executive Summary 1.2 Project Success Criteria 1.3 Assumptions SOLUTION ARCHITECTURE / ARCHITECTURAL DIAGRAM 2.1 Technical Architecture Diagram 2.2 Technical Plan 2.3 Project Plan 2.4 Security Considerations ACTIVITIES AND DELIVERABLES 3.1 Activities and Deliverables 3.2 Out of Scope 3.3 Path to Production EXPECTED AWS COST BREAKDOWN IMPLEMENTATION TEAM RESOURCES \u0026amp; ESTIMATED PERSONNEL COSTS ACCEPTANCE 1. BACKGROUND AND MOTIVATION 1.1 EXECUTIVE SUMMARY Customer Background:\nThe DaiVietBlood system is designed to serve the community, including voluntary blood donors, patients in need of emergency blood, and healthcare professionals in Vietnam. The primary customers are blood donors, patient families, and medical staff responsible for managing blood inventory and donation schedules. They require a centralized, reliable platform to optimize the matching process between donors and recipients and improve communication during emergencies. In the context of digital health transformation, DaiVietBlood provides a secure, accessible solution to address localized blood shortages.\nBusiness and Technical Objectives:\nMigrating the DaiVietBlood system from a local/on-premise environment to AWS offers superior advantages:\nBusiness: AWS allows the application to scale flexibly according to the user base, reduces hardware infrastructure operational costs, and ensures consistent performance nationwide. Technical: AWS provides High Availability and medical data security. Adopting a Serverless Architecture (AWS Lambda, API Gateway, Cognito, RDS) simplifies backend management, accelerates development, and reduces maintenance costs. The system integrates comprehensive monitoring (CloudWatch) and adheres to strict security standards. Summary of Key Use Cases:\nRole Key Function Short Description Guest Access Public Information View donation guidelines, blood compatibility charts, and educational articles without logging in. Member Register/Login, Profile Management Create accounts, update personal information and blood type. Book Blood Donation Select time slots and locations for donation. Submit Emergency Request Submit urgent blood requests; the system automatically finds suitable donors. Staff Manage Requests \u0026amp; Inventory Approve emergency requests, confirm donation schedules, update blood stock. Administrator (Admin) System Administration Manage user accounts, configure donation slots, view overview reports. Summary of Partner‚Äôs Professional Services: The Skyline Team will provide comprehensive digital transformation services, including assessing the current local application, designing a Cloud-native architecture, and executing the migration of the system to an AWS Serverless environment. We commit to delivering a secure, scalable system accompanied by automated CI/CD pipelines and detailed operational documentation.\n1.2 PROJECT SUCCESS CRITERIA Functionality: 100% of core functions (registration, scheduling, emergency requests, administration) operate stably on AWS with no regression errors. Availability: System achieves Uptime ‚â• 99.9%, ensuring continuous 24/7 access. Performance: Application response time improves by at least 30% compared to the local version. Emergency request processing time is reduced by 40%. Cost: Infrastructure costs are optimized by at least 20% thanks to the Serverless model and Auto-scaling. User Experience: UAT acceptance rate reaches a minimum of 95% for all user roles. Security: Full compliance with data encryption, access management (IAM), and API security requirements. Operations: CI/CD pipeline is fully automated with deployment time \u0026lt; 10 minutes. Monitoring system covers 100% of critical services. 1.3 ASSUMPTIONS Technical \u0026amp; Architectural Assumptions:\nSource Code: The current Local application (Frontend \u0026amp; Backend) is functionally complete. The project focuses on Refactoring for the Cloud (Serverless), excluding the development of new features. AWS Region: The entire infrastructure is deployed in Singapore (ap-southeast-1) to optimize latency for users in Vietnam. Note: During the testing phase, due to limited VPC configurations and Free Tier/Student resources, latency may fluctuate (estimated ~3.5s/request). Service Limits: The AWS account uses default limits (Soft limits). Increasing limits to reduce latency will be approved by the Customer when necessary. Third-party Integration: The system uses the Gemini API for AI support features. Access Rights: The Skyline Team is granted Admin access (IAM Role) to provision resources. Operational \u0026amp; Financial Assumptions:\nDomain: The Customer owns the domain name (e.g., daivietblood.com) and DNS configuration rights. Cost: The cost estimate is based on an assumption of approximately 50,000 API requests/month. Actual costs depend on usage levels (Pay-as-you-go). 2. SOLUTION ARCHITECTURE / ARCHITECTURAL DIAGRAM 2.1 TECHNICAL ARCHITECTURE DIAGRAM The DaiVietBlood system utilizes a Serverless-First architecture on AWS Cloud, prioritizing scalability, security, and operational optimization.\nKey Components:\nNetwork Infrastructure (VPC): Public Subnet: Contains Internet Gateway and NAT Gateway. Private Subnet: Contains AWS Lambda and Amazon RDS to isolate and secure data, preventing direct Internet access. Application \u0026amp; Data: Frontend: Hosted on AWS Amplify, distributed via Amazon CloudFront (CDN), and assets stored on S3. Authentication: Amazon Cognito manages identity and issues JWT tokens. API \u0026amp; Compute: Amazon API Gateway receives requests and routes them to AWS Lambda for business logic processing. Database: Amazon RDS stores structured data, located in the Private Subnet. DevOps \u0026amp; Monitoring: CI/CD: Uses AWS CodePipeline, CodeBuild, CodeDeploy to automate the deployment process. Monitoring: Amazon CloudWatch centrally collects logs and metrics. 2.2 TECHNICAL PLAN The technical implementation process follows the Infrastructure-as-Code (IaC) methodology:\nInfrastructure Automation: Use AWS CloudFormation to provision VPC, Lambda, RDS, and API Gateway, ensuring consistency across environments (Dev/Staging/Prod). Application Development: Refactor backend into modular Lambda functions (NodeJS/Python). Environment variables and sensitive information (DB credentials) are securely encrypted. CI/CD Process: Source (GitHub) -\u0026gt; Build (CodeBuild) -\u0026gt; Deploy (CloudFormation/CodeDeploy). Includes a Manual Approval step before deploying to the Production environment. Testing Strategy: Unit Tests for Lambda, Integration Tests for API, and Load Tests to ensure capacity. 2.3 PROJECT PLAN The project applies the Agile Scrum model over 8 weeks (4 Sprints):\nSprint 1 (Foundation): Set up AWS Account, VPC, RDS. Sprint 2 (Backend Core): Develop Lambda, API Gateway, Cognito. Sprint 3 (Integration): Deploy Frontend (Amplify), finalize CI/CD Pipeline. Sprint 4 (Stabilization): UAT, Performance Optimization, Handover. 2.4 SECURITY CONSIDERATIONS Access Management: Use Cognito for user authentication and IAM Roles for service authorization (Least Privilege). Network Isolation: Database and Lambda are located in the Private Subnet, accessing the Internet only via NAT Gateway. Data Protection: Data encryption At-rest (on RDS/S3) and In-transit (via HTTPS). Security Monitoring: CloudWatch Logs record all activities for auditing and intrusion detection. 3. ACTIVITIES AND DELIVERABLES 3.1 ACTIVITIES AND DELIVERABLES Phase Timeline Key Activities Deliverables Estimate (Man-days) Analysis \u0026amp; Design Week 1 Assess Local state, design Cloud architecture, plan migration. SRS Document, Architecture Diagram, API Specs. 5 Local Development Week 2-3 Build backend logic, database schema, local unit tests. Backend Prototype, Database Schema. 10 Frontend \u0026amp; Integration Week 4-5 Develop Frontend, integrate local APIs, prepare code for refactoring. Completed Local Application. 10 AWS Infrastructure Setup Week 6 Write CloudFormation scripts, provision VPC, RDS, IAM. IaC Templates, Secure VPC Environment. 5 Refactor \u0026amp; Deploy Backend Week 7-8 Convert to Lambda, configure API Gateway, Cognito. Serverless Backend active on AWS. 10 Deploy Frontend \u0026amp; CI/CD Week 9-10 Host Frontend on Amplify, set up automated Pipeline. Production URL, CI/CD Pipeline. 10 Testing \u0026amp; Go-live Week 11 UAT, Security Testing, Performance Optimization. UAT Report, Security Report. 5 Handover \u0026amp; Training Week 12 Transfer accounts, operations training, handover documentation. Operations Manual, Acceptance. 5 3.2 OUT OF SCOPE Optimal user search algorithm based on real-time Geo-location (currently using simplified logic). Complex Auto-scaling for the Database layer (currently using basic RDS). Deep Latency Optimization for regions outside Singapore. Advanced Security Compliance standards such as HIPAA/PCI-DSS. 3.3 PATH TO PRODUCTION To upgrade from the current MVP to a large-scale Production system, the following are required:\nEnvironment Strategy: Strictly separate Dev/Staging/Prod environments across different AWS accounts (Multi-account strategy). Database Scaling: Migrate to Amazon Aurora Serverless or use Read Replicas to increase read/write capacity. Enhanced Monitoring: Integrate AWS X-Ray to trace requests and identify performance bottlenecks. Strengthened Security: Deploy AWS WAF with rules to block DDoS and automated bots; use Amazon Inspector for periodic vulnerability scanning. 4. EXPECTED AWS COST BREAKDOWN The following Cost Estimation is based on the Asia Pacific (Singapore) region, which is the standard region for low latency access from Vietnam.\nCategory Service Estimated Configuration Monthly Cost (USD) Network NAT Gateway 1 NAT Gateway (Required for Private Subnet) + Data Processing ~$43.13 VPC Subnets, Security Groups ~$13.14 CloudFront 5GB Data Transfer (Utilizing Free Tier) ~$3.00 Compute Lambda 1,000 requests, 512MB RAM (Free Tier) ~$0.00 API Gateway 1,000 requests ~$0.00 Database RDS db.t3.micro, 20GB Storage ~$21.74 Storage S3 5GB Storage, 200 requests ~$0.14 Hosting Amplify Build \u0026amp; Hosting, WAF enabled ~$16.77 Ops CloudWatch Logs, Metrics, Alarms ~$9.41 CI/CD CodePipeline 1 Active Pipeline ~$1.05 Total ~$108.38 / Month 5. TEAM AWS FCJ Program Lead (Mentor)\nName Title Description Email / Contact Info Nguy·ªÖn Gia H∆∞ng Head of Solutions Architect Provides technical mentorship, architecture review, and AWS best practices guidance. hunggia@amazon.com Project Stakeholders\nName Title Stakeholder for Email / Contact Info AWS FCJ Mentors Program Instructor Academic oversight, project evaluation, internship credit. Internship Team (FPT University)\nName Title Role Email / Contact Info Nguy·ªÖn ƒê·ª©c L√¢n Co-Lead (PM) Project Manager: Responsible for team coordination, progress tracking, code structure, UAT, and Cost Optimization. lannguyen68609@gmail.com Nguy·ªÖn C√¥ng Minh Co-Lead (Technical) DevOps \u0026amp; Backend: CI/CD, CodePipeline, CDK Stack, Lambda implementation. minhncse182968@fpt.edu.vn ƒê·ªó Khang Member Cloud Architect: Architect Design, Service Policy, Co-designed AWS serverless architecture. dokhang307@gmail.com L√™ Ho√†ng Anh Member Fullstack Dev: API implementation, UI/UX design, Security configurations. anhlhse170327@fpt.edu.vn Nguy·ªÖn Qu√°ch Lam Giang Member Data Engineering: RDS with MySQL connection, VPC \u0026amp; Subnet configuration. nguyenlamgiang2198@gmail.com Project Escalation Contacts\nName Title Role Email / Contact Info Nguy·ªÖn ƒê·ª©c L√¢n Team Lead Primary contact for project status and escalations. lannguyen68609@gmail.com 6. RESOURCES \u0026amp; COST ESTIMATES Resource Breakdown\nResource Responsibility Nguy·ªÖn ƒê·ª©c L√¢n Coordination \u0026amp; PM: Project Management, Cost Optimization, Forensic analysis, System configuration, and Full-stack development support. Nguy·ªÖn C√¥ng Minh DevOps \u0026amp; Infrastructure: CI/CD pipeline, Security, CDK Stack, Lambda implementation, Co-design API structure, System infrastructure. L√™ Ho√†ng Anh Frontend \u0026amp; UI/UX: Frontend development, API Integration, UI/UX Design, Application Security. Nguy·ªÖn Qu√°ch Lam Giang Data Engineering: Data Analysis, RDS to MySQL connection, VPC creation, CloudWatch monitoring, Subnet and NAT Gateway configuration. ƒê·ªó Khang Cloud Architecture: Architecture design, Service Policies, Documentation, API structure co-design, AI Chatbot integrations, CloudWatch monitoring. Hours by Project Phase\nProject Phase N.ƒê. L√¢n N.C. Minh L.H. Anh N.Q.L. Giang ƒê. Khang Total Hours Foundation 30 30 30 30 30 150 Core Orchestration 30 30 30 30 30 150 Analytics Layer 30 30 30 30 30 150 Testing \u0026amp; Validation 30 30 30 30 30 150 Documentation \u0026amp; Handover 30 30 30 30 30 150 Total Hours 150 150 150 150 150 750 Cost Distribution\nParty Contribution (USD) % Contribution of Total AWS FCJ Program $0 (Non-profit internship) 0% FPT University Student labor (Academic Credit) 0% AWS Infrastructure ~$15.00 (Testing \u0026amp; Running Costs) 100% Total Project Cost ~$15.00 100% *Note on Cost Efficiency: The labor cost for this project is subsidized as part of the FPT University Internship and AWS First Cloud Journey (FCJ) program. The estimated $15 represents the infrastructure running costs (AWS Credits) required for development and testing environments.\n7. ACCEPTANCE 7.1 Submission of Deliverables: Upon completion of the final phase (\u0026ldquo;Handover\u0026rdquo;), Skyline Team (PROVIDER) will submit the associated tangible Deliverables (Source Code, AWS Architecture Documentation, Admin Credentials, and Operational Manual) to the Project Stakeholders/Instructors (CUSTOMER). This submission will be accompanied by an Acceptance Form (or a formal Acceptance Email).\n7.2 Acceptance Period \u0026amp; Review: Upon such submission, the Customer will review, evaluate, and perform User Acceptance Testing (UAT) on the applicable Deliverable(s) within five (5) business days (the ‚ÄúAcceptance Period‚Äù). The review will determine whether each Deliverable satisfies the Project Success Criteria (defined in Section 1.2) and adheres to the AWS Architecture (defined in Section 2.1) in all material respects.\n7.3 Confirmation of Acceptance: If the Deliverable satisfies its acceptance criteria, the Customer will furnish a written acceptance confirmation to the Provider via the Acceptance Form prior to the end of the Acceptance Period. This signature (or written confirmation) marks the official closure of the project.\n7.4 Rejection \u0026amp; Remediation: For a Deliverable that is not accepted due to a non-conformity (e.g., Critical bugs, Security vulnerabilities, or missing features compared to the Scope of Work), the Customer will indicate the detailed reasons for such rejection on the Acceptance Form (a ‚ÄúRejection Notice‚Äù) within the Acceptance Period.\nUpon receipt of a Rejection Notice, the Provider will promptly correct any defects or non-conformities to the extent required. Thereafter, the Provider will resubmit the modified Deliverable to the Customer, and the acceptance process set forth above will be repeated.\n7.5 Scope of Re-evaluation: The Customer will limit its review of resubmitted Deliverables to determining whether or not the Provider has corrected the defects identified in the Rejection Notice and to the effects which these corrections have on other portions of the system.\n7.6 Deemed Acceptance: If the Customer fails to provide the Provider with the above-described Rejection Notice or signed Acceptance Form prior to the end of the applicable Acceptance Period, then the corresponding Deliverable(s) and the project as a whole are deemed accepted.\n"},{"uri":"https://dokang307.github.io/Internship_Report/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Goals: Mastering Compute Fundamentals: Deep dive into Amazon EC2 architecture, understanding the distinct use cases for different Instance Types. Network Security: Implementing robust firewall rules using Security Groups to control traffic flow. Cross-Platform Deployment: Hands-on practice deploying instances on both Linux and Windows environments using SSH and RDP. Application Setup: Configuring web server environments (LAMP/XAMPP) to simulate real-world application hosting. Tasks to be implemented this week: Day Task Start Date Completion Date Reference 2 Theory \u0026amp; Network Security:\n- Research EC2 core concepts (Instance Types, AMI, EBS, User Data).\n- Configure VPC Security Groups (Inbound/Outbound rules). 22/09/2025 22/09/2025 Module 02-Lab03-04.1 3 Basic Operations:\n- Practice launching EC2 instances in public subnets.\n- Establish connectivity via SSH key pairs. 22/09/2025 23/09/2025 Introduction to Amazon EC2 4 Linux Environment Deep Dive:\n- Deploy Amazon Linux EC2.\n- Advanced Lab: Recover access to an instance after losing the Key Pair.\n- Install LAMP Stack (Apache, MariaDB, PHP). 24/09/2025 24/09/2025 EC2 Linux Guide 5 Windows Environment Setup:\n- Deploy Windows Server instance.\n- Connect via RDP (Remote Desktop Protocol).\n- Install XAMPP solution. 25/09/2025 25/09/2025 EC2 Windows Guide Week 3 Achievement: Mastering EC2 Infrastructure, Security Configuration \u0026amp; Web Server Deployment 1. Deep Dive into EC2 Architecture \u0026amp; Components Compute Service Model: Gained a solid understanding of EC2 as an IaaS solution for on-demand scalable computing. Core Components: Analyzed the ecosystem required to run an instance effectively: Instance Types: Selection based on workload (CPU, Memory, Storage optimized). AMI (Amazon Machine Image): Using pre-configured templates for rapid deployment. Storage: Distinguished between EBS (persistent block storage) and Instance Store. Bootstrapping: Utilized User Data to run automated scripts upon instance initialization. 2. Network Security Implementation Security Group Management: Configured Security Groups as a stateful firewall at the instance level. Defined precise Inbound rules to allow necessary services (SSH port 22, HTTP port 80, RDP port 3389). Understood the default behavior of Outbound rules and connection tracking. 3. Linux Administration \u0026amp; Troubleshooting Connectivity: Mastered SSH connections using private keys (.pem). Disaster Recovery (Key Pair Loss): Successfully performed a manual recovery of a \u0026ldquo;lost\u0026rdquo; instance by: Creating a fresh Key Pair. Utilizing PuTTYgen to extract the Public Key. Injecting the new Public Key into the instance\u0026rsquo;s user-data script (cloud-config) while the instance is stopped. Validating access restoration after a reboot. Web Stack Deployment: Successfully installed and configured the LAMP stack (Linux, Apache, MariaDB, PHP) to host a test web page. 4. Windows Server Administration Access Management: Retrieved the Administrator password by decrypting it with the Key Pair file. Remote Access: Used Microsoft Remote Desktop (RDP/mstsc) for GUI-based management. Application Layer: Deployed XAMPP to verify web server functionality on a Windows environment. 5. Challenges \u0026amp; Solutions Issue 1 (Connectivity): SSH connection timed out initially. Solution: Audited the Security Group and added an Inbound Rule for port 22 from \u0026ldquo;My IP\u0026rdquo;. Issue 2 (Software Installation): Apache installation failed on Amazon Linux 2023 because the amazon-linux-extras command is deprecated. Solution: Researched documentation and switched to using dnf and specific distro-stream repositories compatible with AL2023. "},{"uri":"https://dokang307.github.io/Internship_Report/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/","title":"Test API Endpoints","tags":[],"description":"","content":"Step 1: Test from API Gateway Console 1.1. Test GET /users\nGo to API Gateway Console ‚Üí Select daivietblood-api Select /users ‚Üí GET Click Test Click Test button Expected response:\n{ \u0026#34;statusCode\u0026#34;: 200, \u0026#34;body\u0026#34;: \u0026#34;[]\u0026#34; } Step 2: Test with cURL Replace YOUR_API_URL with your actual Invoke URL.\n2.1. Create a User (POST /users)\ncurl -X POST https://YOUR_API_URL/prod/users \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;email\u0026#34;: \u0026#34;nguyen.van.a@example.com\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Nguyen Van A\u0026#34;, \u0026#34;blood_type\u0026#34;: \u0026#34;O+\u0026#34;, \u0026#34;phone\u0026#34;: \u0026#34;0901234567\u0026#34; }\u0026#39; Expected response:\n{ \u0026#34;id\u0026#34;: 1, \u0026#34;email\u0026#34;: \u0026#34;nguyen.van.a@example.com\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Nguyen Van A\u0026#34;, \u0026#34;blood_type\u0026#34;: \u0026#34;O+\u0026#34;, \u0026#34;phone\u0026#34;: \u0026#34;0901234567\u0026#34; } 2.2. Get All Users (GET /users)\ncurl https://YOUR_API_URL/prod/users Expected response:\n[ { \u0026#34;id\u0026#34;: 1, \u0026#34;email\u0026#34;: \u0026#34;nguyen.van.a@example.com\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Nguyen Van A\u0026#34;, \u0026#34;blood_type\u0026#34;: \u0026#34;O+\u0026#34;, \u0026#34;phone\u0026#34;: \u0026#34;0901234567\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2025-12-09T10:00:00.000Z\u0026#34; } ] 2.3. Create Emergency Request (POST /emergency-requests)\ncurl -X POST https://YOUR_API_URL/prod/emergency-requests \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;requester_name\u0026#34;: \u0026#34;Benh vien Cho Ray\u0026#34;, \u0026#34;blood_type\u0026#34;: \u0026#34;AB-\u0026#34;, \u0026#34;units_needed\u0026#34;: 5, \u0026#34;hospital\u0026#34;: \u0026#34;Cho Ray Hospital\u0026#34;, \u0026#34;urgency\u0026#34;: \u0026#34;critical\u0026#34; }\u0026#39; Expected response:\n{ \u0026#34;id\u0026#34;: 1, \u0026#34;message\u0026#34;: \u0026#34;Emergency request created\u0026#34; } 2.4. Get Emergency Requests (GET /emergency-requests)\ncurl https://YOUR_API_URL/prod/emergency-requests Step 3: Test with Postman Open Postman Create new Collection: DaiVietBlood API Add requests: Request Name Method URL Get Users GET {{baseUrl}}/users Create User POST {{baseUrl}}/users Get Emergency Requests GET {{baseUrl}}/emergency-requests Create Emergency Request POST {{baseUrl}}/emergency-requests Set Collection variable: baseUrl: https://YOUR_API_URL/prod Step 4: Verify Lambda Logs Go to CloudWatch Console ‚Üí Log groups\nFind log groups:\n/aws/lambda/daivietblood-get-users /aws/lambda/daivietblood-create-user /aws/lambda/daivietblood-emergency-requests Check recent log streams for:\nSuccessful invocations Any errors or exceptions Database connection logs Common Issues \u0026amp; Solutions Issue Cause Solution 502 Bad Gateway Lambda error Check CloudWatch logs for details Timeout Lambda cannot reach RDS Verify VPC, Subnets, Security Groups CORS error CORS not configured Enable CORS on API Gateway 500 Internal Server Error Database connection failed Check DB credentials in environment variables Step 5: Performance Check Note the response time for each API call First call may be slow (Lambda cold start) Subsequent calls should be faster Expected performance:\nEndpoint Cold Start Warm GET /users ~3-5s ~200-500ms POST /users ~3-5s ~200-500ms GET /emergency-requests ~3-5s ~200-500ms üí° Tip: Lambda cold start in VPC can be slow. Consider using Provisioned Concurrency for production workloads.\nVerification Checklist GET /users returns empty array or user list POST /users creates new user successfully GET /emergency-requests returns requests list POST /emergency-requests creates new request No CORS errors in browser console CloudWatch logs show successful invocations "},{"uri":"https://dokang307.github.io/Internship_Report/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nThis section will list and introduce the blogs you have translated. For example:\nBlog 1 - Optimizing SAP Operations with CloudWatch MCP server and Amazon Q CLI ‚Äì Part 3 Here is the summary in English:\nThis blog introduces a solution for optimizing SAP operations on AWS by integrating Amazon CloudWatch MCP Server and Amazon Q CLI, effectively addressing challenges such as complex command syntax and manual troubleshooting. By enabling engineers to interact with the system using natural language, Amazon Q CLI automatically translates requests into precise technical commands, while the CloudWatch MCP Server provides real-time data context (metrics and logs) to execute complex tasks like automated health reporting and routine maintenance. This powerful combination not only simplifies monitoring and accelerates incident resolution in alignment with the AWS Well-Architected Framework but also significantly boosts operational productivity for SAP administration teams.\nBlog 2 - Streamlining SAP Operations with CloudWatch MCP Server and Amazon Q CLI ‚Äì Part 4 This article, the fourth installment in the series on optimizing SAP operations, focuses on the advanced application of Amazon CloudWatch MCP Server and Amazon Q CLI to streamline planned maintenance and accelerate Root-Cause Analysis (RCA). Through practical scenarios‚Äîsuch as coordinating cluster maintenance and resolving complex ERS service failures caused by configuration errors‚Äîthe post demonstrates how this integration enables engineers to diagnose issues, propose solutions, and execute precise remediation commands using natural language without the need for context switching. By unifying observability data from both SAP and AWS to minimize Mean Time To Resolution (MTTR), the solution not only bridges the knowledge gap between SAP Basis and infrastructure teams but also significantly enhances operational productivity and system reliability at a reasonable implementation cost.\nBlog 3 - Deploy Okta as a Custom Identity Provider for AWS Transfer Family Here is the summary in English:\nThis blog details the implementation of Okta as a Custom Identity Provider (IdP) for AWS Transfer Family to address the challenges of managing granular data access and security in growing organizations. Utilizing a modular architecture powered by Amazon DynamoDB and AWS Lambda, the solution enables robust Okta authentication with Multi-Factor Authentication (MFA) support while offering precise control over individual user sessions, including specific home directory mappings and IAM roles. Through a step-by-step walkthrough covering Okta configuration, AWS toolkit deployment, and connection testing, the post provides a standardized, scalable framework for building secure SFTP environments that integrate seamlessly with centralized identity management systems.\n"},{"uri":"https://dokang307.github.io/Internship_Report/5-workshop/5.3-s3-vpc/","title":"VPC &amp; Amazon RDS","tags":[],"description":"","content":"In this section, you will create the network infrastructure (VPC) and database (RDS) for the DaiVietBlood system.\nArchitecture Overview Content Create VPC Create Amazon RDS "},{"uri":"https://dokang307.github.io/Internship_Report/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Goals: Storage \u0026amp; Content Delivery: Understand and practice deploying static website hosting using Amazon S3 combined with the CloudFront content delivery network. Developer Tools (DevTools): Get familiar with using AWS CLI via VSCode to interact with AWS services more professionally and efficiently. Databases: Practice creating and managing relational databases with Amazon RDS (MySQL). Tasks to be implemented this week: Day Task Start Date Completion Date Reference 2 S3 \u0026amp; CloudFront:\n- Research core concepts of S3 and CloudFront.\n- Practice: Configure an S3 bucket for static website hosting, manage access permissions, create a CloudFront distribution, and practice Bucket Versioning. 29/09/2025 30/09/2025 Amazon S3 Guide 3 AWS CLI \u0026amp; VSCode:\n- Setup development environment: Connect AWS with VSCode.\n- Use AWS CLI to interact with services instead of using the Console. 30/09/2025 30/09/2025 AWS CLI Configuration 4 Databases:\n- Practice creating an RDS Instance.\n- Connect and work with MySQL on AWS infrastructure. 01/10/2025 01/10/2025 Amazon RDS (MySQL) Week 4 Achievement: Static Website, CDN \u0026amp; Database Administration 1. S3 Storage Mechanism and Static Website Hosting Understanding S3: Mastered S3 as an Object Storage service, identifying it as an ideal solution for hosting static web resources (HTML/CSS/JS). Deployment Process: Enabled the Static Website Hosting feature on the Bucket. Configured key documents: index.html (entry point) and error.html (error page). Access Management: Configured Bucket Policy to allow public read access or restricted access via CloudFront for enhanced security. Successfully tested website access via the S3 Endpoint URL. 2. Optimizing Content Delivery with CloudFront Distribution Configuration: Origin: Pointed to the S3 bucket containing the web source code. Behavior: Configured caching policies, Time-to-Live (TTL), and HTTP/HTTPS protocols. Key Benefits: Performance: Accelerated page load speeds leveraging the Edge Location network. Security: Hidden the origin S3 bucket from end-users (users interact only with CloudFront), supporting HTTPS via ACM Certificates. Operations: Understood how to use Invalidations to clear old cache when updating new website content. 3. Professional Interaction with AWS CLI \u0026amp; VSCode Environment Setup: Successfully installed the AWS Toolkit extension for VSCode. Configured user authentication (aws configure) using Access Key and Secret Key. Command Line Operations: Transitioned from using the graphical interface (Console) to using CLI commands for managing resources like S3, EC2, and RDS, significantly speeding up workflow. 4. Amazon RDS (MySQL) Administration Database Deployment: Successfully launched an RDS MySQL instance. Network Configuration: Set up Subnet Groups and Security Groups to control traffic flow to the database. Connection \u0026amp; Querying: Used MySQL Workbench to connect to the RDS endpoint. Performed basic database administration tasks (Creating DBs, executing SQL commands). "},{"uri":"https://dokang307.github.io/Internship_Report/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/","title":"Configure CORS &amp; Security","tags":[],"description":"","content":"Understanding CORS CORS (Cross-Origin Resource Sharing) is a security feature that restricts web pages from making requests to a different domain than the one serving the web page.\nWhen your React frontend (hosted on Amplify) calls your API Gateway, the browser checks CORS headers to determine if the request is allowed.\nStep 1: Configure CORS Headers in Lambda Ensure all Lambda functions return proper CORS headers:\nconst corsHeaders = { \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39;, // Or specific domain \u0026#39;Access-Control-Allow-Headers\u0026#39;: \u0026#39;Content-Type,X-Amz-Date,Authorization,X-Api-Key,X-Amz-Security-Token\u0026#39;, \u0026#39;Access-Control-Allow-Methods\u0026#39;: \u0026#39;GET,POST,PUT,DELETE,OPTIONS\u0026#39; }; // In your handler response: return { statusCode: 200, headers: corsHeaders, body: JSON.stringify(data) }; Step 2: Configure API Gateway CORS Method 1: Using Console\nGo to API Gateway Console ‚Üí Select your API For each resource: Select resource ‚Üí Actions ‚Üí Enable CORS Configure allowed origins, methods, headers Click Enable CORS and replace existing CORS headers Method 2: Using OPTIONS Method\nCreate OPTIONS method for each resource Integration type: Mock Add Method Response with status 200 Add Integration Response with headers: Access-Control-Allow-Headers: \u0026#39;Content-Type,X-Amz-Date,Authorization,X-Api-Key\u0026#39; Access-Control-Allow-Methods: \u0026#39;GET,POST,OPTIONS\u0026#39; Access-Control-Allow-Origin: \u0026#39;*\u0026#39; Step 3: API Gateway Security Best Practices 3.1. Enable API Key (Optional)\nGo to API Gateway ‚Üí API Keys ‚Üí Create API Key Name: daivietblood-api-key Go to Usage Plans ‚Üí Create Configure throttling and quota Associate API Key with Usage Plan For each method, set API Key Required: true 3.2. Enable Request Validation\nGo to API Gateway ‚Üí Models ‚Üí Create Create model for request body: { \u0026#34;$schema\u0026#34;: \u0026#34;http://json-schema.org/draft-04/schema#\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;CreateUserModel\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;required\u0026#34;: [\u0026#34;email\u0026#34;, \u0026#34;name\u0026#34;, \u0026#34;blood_type\u0026#34;], \u0026#34;properties\u0026#34;: { \u0026#34;email\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;email\u0026#34; }, \u0026#34;name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;minLength\u0026#34;: 1 }, \u0026#34;blood_type\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;enum\u0026#34;: [\u0026#34;A+\u0026#34;, \u0026#34;A-\u0026#34;, \u0026#34;B+\u0026#34;, \u0026#34;B-\u0026#34;, \u0026#34;AB+\u0026#34;, \u0026#34;AB-\u0026#34;, \u0026#34;O+\u0026#34;, \u0026#34;O-\u0026#34;] }, \u0026#34;phone\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } } } Apply model to POST method: Select method ‚Üí Method Request Request Validator: Validate body Request Body: Add model 3.3. Enable Throttling\nGo to Stages ‚Üí Select prod Stage Settings ‚Üí Default Method Throttling Configure: Rate: 100 requests/second Burst: 200 requests Step 4: Lambda Security Best Practices 4.1. Use AWS Secrets Manager for Credentials\nInstead of storing DB credentials in environment variables:\nGo to Secrets Manager ‚Üí Store a new secret\nSecret type: Other type of secret\nKey/value pairs:\nDB_HOST: daivietblood-db.xxxx.rds.amazonaws.com DB_USER: admin DB_PASSWORD: YourSecurePassword123! DB_NAME: daivietblood Secret name: daivietblood/db-credentials\nUpdate Lambda to retrieve secrets:\nconst { SecretsManagerClient, GetSecretValueCommand } = require(\u0026#39;@aws-sdk/client-secrets-manager\u0026#39;); const client = new SecretsManagerClient({ region: \u0026#39;ap-southeast-1\u0026#39; }); const getDbCredentials = async () =\u0026gt; { const command = new GetSecretValueCommand({ SecretId: \u0026#39;daivietblood/db-credentials\u0026#39; }); const response = await client.send(command); return JSON.parse(response.SecretString); }; Add IAM permission to Lambda role: { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;secretsmanager:GetSecretValue\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:secretsmanager:ap-southeast-1:*:secret:daivietblood/*\u0026#34; } 4.2. Input Validation\nAlways validate input in Lambda:\nconst validateUser = (body) =\u0026gt; { const errors = []; if (!body.email || !isValidEmail(body.email)) { errors.push(\u0026#39;Invalid email\u0026#39;); } if (!body.name || body.name.length \u0026lt; 1) { errors.push(\u0026#39;Name is required\u0026#39;); } const validBloodTypes = [\u0026#39;A+\u0026#39;, \u0026#39;A-\u0026#39;, \u0026#39;B+\u0026#39;, \u0026#39;B-\u0026#39;, \u0026#39;AB+\u0026#39;, \u0026#39;AB-\u0026#39;, \u0026#39;O+\u0026#39;, \u0026#39;O-\u0026#39;]; if (!validBloodTypes.includes(body.blood_type)) { errors.push(\u0026#39;Invalid blood type\u0026#39;); } return errors; }; Step 5: Redeploy API After making changes:\nActions ‚Üí Deploy API Select prod stage Click Deploy Security Checklist CORS configured correctly Lambda returns proper CORS headers API Key enabled (optional but recommended) Request validation enabled Throttling configured DB credentials stored in Secrets Manager (recommended) Input validation in Lambda functions API redeployed after changes "},{"uri":"https://dokang307.github.io/Internship_Report/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: Cloud Day Vietnam\nDate \u0026amp; Time: 09:00, September 18, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: Data Resiliency in a Cloud - First World\nDate \u0026amp; Time: 09:00, October 14, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 3 Event Name: Data Science on AWS\nDate \u0026amp; Time: 09:00, October 16, 2025\nLocation: Hall B, FPT University Ho Chi Minh, District 9, Thu Duc, Ho Chi Minh\nRole: Attendee\nEvent 4 Event Name: AWS Cloud Mastery Series #1\nDate \u0026amp; Time: 09:00, November 15, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 5 Event Name: AWS Cloud Mastery Series #2\nDate \u0026amp; Time: 09:00, November 17, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 6 Event Name: AWS Cloud Mastery Series #3\nDate \u0026amp; Time: 09:00, November 29, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"},{"uri":"https://dokang307.github.io/Internship_Report/5-workshop/5.4-s3-onprem/","title":"Lambda &amp; API Gateway","tags":[],"description":"","content":"In this section, you will create AWS Lambda functions and expose them via Amazon API Gateway to build the serverless backend for DaiVietBlood.\nArchitecture Overview API Endpoints Method Endpoint Description GET /users Get all users POST /users Create new user GET /users/{id} Get user by ID GET /donations Get all donations POST /donations Create donation appointment GET /emergency-requests Get emergency requests POST /emergency-requests Create emergency request Content Create Lambda Functions Create API Gateway Test API Endpoints Configure CORS \u0026amp; Security "},{"uri":"https://dokang307.github.io/Internship_Report/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Goals: Tasks to be implemented this week: Day Task Start Date Completion Date Reference 2 - Study Module 5 theory: Security services on AWS\n- Shared Responsibility Model\n- Amazon IAM\n- AWS Cognito\n- AWS Organizations\n- AWS Identity Center\n- AWS KMS\n- AWS Security Hub 06/10/2025 06/10/2025 - IAM\n- Cognito\n- KMS 3 - Discuss and finalize group project\n- Services to be used\n- Data\n- Features 07/10/2025 07/10/2025 Meeting Minutes 4 - Research serverless services (features, pricing, \u0026hellip;) 08/10/2025 08/10/2025 Service 5 09/10/2025 09/10/2025 6 10/10/2025 10/10/2025 Week 5 achievement: Governance, Security, and Serverless Mindset AWS Security \u0026amp; Identity Management: Reinforced knowledge of the Shared Responsibility Model and identity management tools (Cognito, Identity Center). Mastered the roles of KMS and Security Hub in ensuring overall security posture. Serverless Architecture Research: Completed in-depth research on the Serverless model, focusing on Lambda, API Gateway, and DynamoDB. This is the foundation for the group project deployment. Capstone Project Initiation: Worked effectively as a team to finalize the architecture (using Serverless), assign tasks, and set up the initial development environment. The project will be an opportunity to apply all learned knowledge practically. Governance: Began understanding AWS Organizations and SCPs (Service Control Policies) as a means to manage and control resources across multiple accounts, preparing for larger scale. "},{"uri":"https://dokang307.github.io/Internship_Report/5-workshop/5.5-policy/","title":"S3, CloudFront &amp; Amplify","tags":[],"description":"","content":"In this section, you will set up Amazon S3 for static assets, CloudFront for content distribution, and AWS Amplify to host the React frontend application.\nArchitecture Overview Part 1: Amazon S3 Setup Step 1: Create S3 Bucket for Assets Go to S3 Console ‚Üí Create bucket\nGeneral configuration:\nBucket name: daivietblood-assets-{your-account-id} AWS Region: Asia Pacific (Singapore) ap-southeast-1 Object Ownership:\nACLs disabled (recommended) Block Public Access settings:\nBlock all public access: ‚úÖ (We\u0026rsquo;ll use CloudFront) Bucket Versioning:\nEnable (recommended for production) Default encryption:\nServer-side encryption: Enable Encryption type: Amazon S3 managed keys (SSE-S3) Click Create bucket\nStep 2: Create Bucket Policy for CloudFront After creating CloudFront distribution (Part 2), update bucket policy:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AllowCloudFrontAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;cloudfront.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::daivietblood-assets-{your-account-id}/*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;AWS:SourceArn\u0026#34;: \u0026#34;arn:aws:cloudfront::{account-id}:distribution/{distribution-id}\u0026#34; } } } ] } Step 3: Upload Sample Assets Create folder structure:\n/images /blood-types /icons /banners /documents Upload sample images for the application\nPart 2: CloudFront Setup Step 1: Create CloudFront Distribution Go to CloudFront Console ‚Üí Create distribution\nOrigin settings:\nOrigin domain: Select your S3 bucket Origin path: Leave empty Name: daivietblood-s3-origin Origin access: Origin access control settings (recommended) Create new OAC: Click Create control setting Name: daivietblood-oac Signing behavior: Sign requests Default cache behavior:\nViewer protocol policy: Redirect HTTP to HTTPS Allowed HTTP methods: GET, HEAD Cache policy: CachingOptimized Settings:\nPrice class: Use only North America and Europe (or All edge locations) Default root object: index.html Click Create distribution\nImportant: Copy the bucket policy provided and update your S3 bucket policy\nStep 2: Get CloudFront Domain After distribution is deployed (takes 5-10 minutes):\nCopy the Distribution domain name:\nhttps://d1234567890.cloudfront.net Test accessing an asset:\nhttps://d1234567890.cloudfront.net/images/logo.png Part 3: AWS Amplify Setup Step 1: Prepare React Application Create React app (if not exists): npx create-react-app daivietblood-frontend cd daivietblood-frontend Install dependencies: npm install axios react-router-dom Create .env file: REACT_APP_API_URL=https://xxxxxxxxxx.execute-api.ap-southeast-1.amazonaws.com/prod REACT_APP_ASSETS_URL=https://d1234567890.cloudfront.net Sample API service (src/services/api.js): import axios from \u0026#39;axios\u0026#39;; const API_URL = process.env.REACT_APP_API_URL; export const getUsers = async () =\u0026gt; { const response = await axios.get(`${API_URL}/users`); return response.data; }; export const createUser = async (userData) =\u0026gt; { const response = await axios.post(`${API_URL}/users`, userData); return response.data; }; export const getEmergencyRequests = async () =\u0026gt; { const response = await axios.get(`${API_URL}/emergency-requests`); return response.data; }; export const createEmergencyRequest = async (requestData) =\u0026gt; { const response = await axios.post(`${API_URL}/emergency-requests`, requestData); return response.data; }; Push to GitHub repository Step 2: Deploy with Amplify Go to AWS Amplify Console ‚Üí Create new app\nChoose source:\nGitHub ‚Üí Continue Authorize AWS Amplify to access your GitHub Add repository branch:\nRepository: Select your repository Branch: main Configure build settings:\nApp name: daivietblood-frontend Build and test settings: Auto-detected for React Build settings (amplify.yml):\nversion: 1 frontend: phases: preBuild: commands: - npm ci build: commands: - npm run build artifacts: baseDirectory: build files: - \u0026#39;**/*\u0026#39; cache: paths: - node_modules/**/* Environment variables:\nAdd REACT_APP_API_URL and REACT_APP_ASSETS_URL Click Save and deploy\nStep 3: Configure Custom Domain (Optional) Go to App settings ‚Üí Domain management Click Add domain Enter your domain name Configure DNS records as instructed Part 4: Verify Deployment Access Amplify URL:\nhttps://main.d1234567890.amplifyapp.com Test functionality:\nHomepage loads correctly API calls work (check Network tab) Images load from CloudFront No CORS errors Verification Checklist S3 bucket created with proper settings CloudFront distribution deployed S3 bucket policy updated for CloudFront access Assets accessible via CloudFront URL React app deployed to Amplify Environment variables configured Frontend can call API Gateway Images load from CloudFront "},{"uri":"https://dokang307.github.io/Internship_Report/5-workshop/","title":"Workshop","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nBuilding Serverless System on AWS - DaiVietBlood Overview This workshop guides you through building a Serverless Blood Donation \u0026amp; Emergency System (DaiVietBlood) on AWS. You will learn how to set up and configure the core AWS services used in the project architecture.\nAWS Services Used Service Purpose Amazon VPC Create virtual private network with Public/Private Subnets NAT Gateway Allow resources in Private Subnet to access Internet Amazon RDS MySQL database for the application AWS Lambda Serverless business logic processing Amazon API Gateway Manage and expose REST APIs Amazon S3 Store static assets (images, files) Amazon CloudFront CDN for global content distribution AWS Amplify Host Frontend application (React) AWS CodePipeline CI/CD automation Amazon CloudWatch Monitoring and logging What You Will Learn Design and deploy Serverless-First architecture on AWS Configure VPC with Public/Private Subnets for security Create RDS MySQL in Private Subnet Build Lambda Functions and connect with API Gateway Store and distribute content with S3 and CloudFront Deploy React application with AWS Amplify Set up automated CI/CD Pipeline Monitor application with CloudWatch Prerequisites AWS Account with Administrator access Basic knowledge of AWS services Familiarity with Node.js and React AWS CLI installed and configured Estimated Cost This workshop uses resources within AWS Free Tier when possible. Estimated cost is approximately ~$15-20 if completed within 1-2 days and resources are cleaned up immediately after.\nContent Workshop Overview Preparation VPC \u0026amp; Amazon RDS Lambda \u0026amp; API Gateway S3, CloudFront \u0026amp; Amplify CI/CD, CloudWatch \u0026amp; Cleanup "},{"uri":"https://dokang307.github.io/Internship_Report/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Goals: Tasks to be implemented this week: Day Task Start Date Completion Date Reference 2 Start drawing AWS architecture diagrams 13/10/2025 13/10/2025 Architecture Diagram 3 Attend event \u0026ldquo;Data Resiliency in a Cloud-first World\u0026rdquo; 14/10/2025 14/10/2025 4 Write documentation for the proposal 15/10/2025 15/10/2025 Document Link 5 Attend event \u0026ldquo;Data Science on AWS\u0026rdquo; 16/10/2025 16/10/2025 6 17/10/2025 17/10/2025 Week 6 Harvest: Finalizing Solution Design and Data Orientation Project Architecture \u0026amp; Design: Transformed ideas into detailed AWS architecture diagrams. Completed the official Project Proposal, including technology decisions (Serverless) and non-functional requirements. Data Resiliency: Understood the importance of data resiliency following the data center incident in Korea. "},{"uri":"https://dokang307.github.io/Internship_Report/5-workshop/5.6-cleanup/","title":"CI/CD, CloudWatch &amp; Cleanup","tags":[],"description":"","content":"In this final section, you will set up CI/CD Pipeline, configure CloudWatch monitoring, and clean up all resources after completing the workshop.\nPart 1: CI/CD Pipeline with CodePipeline Step 1: Create CodeBuild Project Go to CodeBuild Console ‚Üí Create build project\nProject configuration:\nProject name: daivietblood-backend-build Description: Build project for Lambda functions Source:\nSource provider: GitHub Repository: Select your repository Branch: main Environment:\nEnvironment image: Managed image Operating system: Amazon Linux 2 Runtime: Standard Image: aws/codebuild/amazonlinux2-x86_64-standard:4.0 Service role: New service role Buildspec:\nBuild specifications: Use a buildspec file Create buildspec.yml file in your repository: version: 0.2 phases: install: runtime-versions: nodejs: 18 commands: - echo Installing dependencies... - cd backend \u0026amp;\u0026amp; npm ci pre_build: commands: - echo Running tests... - npm test || true build: commands: - echo Building Lambda packages... - mkdir -p dist - zip -r dist/get-users.zip functions/get-users/ - zip -r dist/create-user.zip functions/create-user/ - zip -r dist/emergency-requests.zip functions/emergency-requests/ post_build: commands: - echo Updating Lambda functions... - aws lambda update-function-code --function-name daivietblood-get-users --zip-file fileb://dist/get-users.zip - aws lambda update-function-code --function-name daivietblood-create-user --zip-file fileb://dist/create-user.zip - aws lambda update-function-code --function-name daivietblood-emergency-requests --zip-file fileb://dist/emergency-requests.zip artifacts: files: - dist/**/* Click Create build project Step 2: Create CodePipeline Go to CodePipeline Console ‚Üí Create pipeline\nPipeline settings:\nPipeline name: daivietblood-pipeline Service role: New service role Source stage:\nSource provider: GitHub (Version 2) Connection: Create new connection or select existing Repository name: Select your repository Branch name: main Output artifact format: CodePipeline default Build stage:\nBuild provider: AWS CodeBuild Project name: daivietblood-backend-build Deploy stage:\nSkip deploy stage (Lambda is updated in build stage) Click Create pipeline\nStep 3: Add IAM Permissions for CodeBuild Go to IAM Console ‚Üí Roles Find role codebuild-daivietblood-backend-build-service-role Add inline policy: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;lambda:UpdateFunctionCode\u0026#34;, \u0026#34;lambda:GetFunction\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:lambda:ap-southeast-1:*:function:daivietblood-*\u0026#34; } ] } Part 2: CloudWatch Monitoring Step 1: Create CloudWatch Dashboard Go to CloudWatch Console ‚Üí Dashboards ‚Üí Create dashboard\nDashboard name: DaiVietBlood-Monitoring\nAdd widgets:\nWidget 1: Lambda Invocations\nWidget type: Line Metrics: Lambda ‚Üí By Function Name ‚Üí Invocations Select all daivietblood functions Widget 2: Lambda Errors\nWidget type: Number Metrics: Lambda ‚Üí By Function Name ‚Üí Errors Statistic: Sum Widget 3: Lambda Duration\nWidget type: Line Metrics: Lambda ‚Üí By Function Name ‚Üí Duration Statistic: Average Widget 4: API Gateway Requests\nWidget type: Line Metrics: ApiGateway ‚Üí By Api Name ‚Üí Count Widget 5: RDS Connections\nWidget type: Line Metrics: RDS ‚Üí Per-Database Metrics ‚Üí DatabaseConnections Step 2: Create CloudWatch Alarms Alarm 1: Lambda Errors\nGo to CloudWatch ‚Üí Alarms ‚Üí Create alarm Select metric: Lambda ‚Üí By Function Name ‚Üí Errors Conditions: Threshold type: Static Whenever Errors is: Greater than 5 Period: 5 minutes Notification: Create new SNS topic: daivietblood-alerts Email: your-email@example.com Alarm name: DaiVietBlood-Lambda-Errors Alarm 2: RDS CPU High\nCreate alarm Select metric: RDS ‚Üí Per-Database Metrics ‚Üí CPUUtilization Conditions: Threshold: Greater than 80% Period: 5 minutes Notification: Use existing SNS topic Alarm name: DaiVietBlood-RDS-CPU-High Alarm 3: API Gateway 5XX Errors\nCreate alarm Select metric: ApiGateway ‚Üí By Api Name ‚Üí 5XXError Conditions: Threshold: Greater than 10 Period: 5 minutes Alarm name: DaiVietBlood-API-5XX-Errors Step 3: Configure Log Insights Go to CloudWatch ‚Üí Logs ‚Üí Logs Insights\nSelect log groups:\n/aws/lambda/daivietblood-get-users /aws/lambda/daivietblood-create-user /aws/lambda/daivietblood-emergency-requests Sample query - Find errors:\nfields @timestamp, @message | filter @message like /ERROR/ | sort @timestamp desc | limit 50 Sample query - Duration statistics: fields @timestamp, @duration | stats avg(@duration), max(@duration), min(@duration) by bin(1h) Part 3: Resource Cleanup ‚ö†Ô∏è Important: Follow these steps to avoid unexpected charges.\nCleanup Order (Important!) Clean up in the following order to avoid dependency errors:\nStep 1: Delete Amplify App Go to Amplify Console Select daivietblood-frontend Actions ‚Üí Delete app Confirm deletion Step 2: Delete CloudFront Distribution Go to CloudFront Console Select distribution ‚Üí Disable Wait for status to change to \u0026ldquo;Deployed\u0026rdquo; Select distribution ‚Üí Delete Step 3: Delete S3 Buckets Go to S3 Console Select bucket daivietblood-assets-* Empty bucket first Then Delete bucket Step 4: Delete API Gateway Go to API Gateway Console Select daivietblood-api Actions ‚Üí Delete Step 5: Delete Lambda Functions Go to Lambda Console Delete each function: daivietblood-get-users daivietblood-create-user daivietblood-emergency-requests Delete Lambda Layer: mysql2-layer Step 6: Delete RDS Instance Go to RDS Console ‚Üí Databases Select daivietblood-db Actions ‚Üí Delete Uncheck \u0026ldquo;Create final snapshot\u0026rdquo; Check \u0026ldquo;I acknowledge\u0026hellip;\u0026rdquo; Type delete me to confirm Step 7: Delete VPC Resources Go to VPC Console\nDelete NAT Gateway:\nNAT Gateways ‚Üí Select NAT Gateway ‚Üí Delete Wait for status \u0026ldquo;Deleted\u0026rdquo; Release Elastic IP:\nElastic IPs ‚Üí Select EIP ‚Üí Release Delete VPC Endpoints (if any):\nEndpoints ‚Üí Select endpoints ‚Üí Delete Delete Security Groups (except default):\nSecurity Groups ‚Üí Delete daivietblood-lambda-sg, daivietblood-rds-sg Delete DB Subnet Group:\nRDS Console ‚Üí Subnet groups ‚Üí Delete daivietblood-db-subnet-group Delete VPC:\nYour VPCs ‚Üí Select daivietblood-vpc ‚Üí Delete VPC This will delete subnets, route tables, internet gateway Step 8: Delete CI/CD Resources CodePipeline Console ‚Üí Delete daivietblood-pipeline CodeBuild Console ‚Üí Delete daivietblood-backend-build Step 9: Delete CloudWatch Resources CloudWatch ‚Üí Dashboards ‚Üí Delete DaiVietBlood-Monitoring CloudWatch ‚Üí Alarms ‚Üí Delete all related alarms CloudWatch ‚Üí Log groups ‚Üí Delete log groups /aws/lambda/daivietblood-* Step 10: Delete IAM Resources IAM Console ‚Üí Roles Delete roles: daivietblood-lambda-role codebuild-daivietblood-* codepipeline-daivietblood-* Cleanup Checklist Amplify app deleted CloudFront distribution deleted S3 buckets emptied and deleted API Gateway deleted Lambda functions and layers deleted RDS instance deleted NAT Gateway deleted Elastic IP released VPC and all components deleted CodePipeline and CodeBuild deleted CloudWatch dashboards, alarms, log groups deleted IAM roles deleted Verify No Remaining Charges Go to AWS Cost Explorer Verify no resources are running Go to Billing Console ‚Üí Bills to confirm üí° Tip: Set up Budget Alert in AWS Budgets to receive notifications when costs exceed threshold.\nWorkshop Conclusion Congratulations! üéâ You have completed the workshop on building a Serverless system on AWS.\nWhat You Learned: ‚úÖ Design and deploy VPC with Public/Private Subnets ‚úÖ Create RDS MySQL in a secure environment ‚úÖ Build Lambda functions and expose via API Gateway ‚úÖ Configure S3 and CloudFront for static assets ‚úÖ Deploy React app with AWS Amplify ‚úÖ Set up automated CI/CD Pipeline ‚úÖ Monitor application with CloudWatch Next Steps: Learn more about AWS Well-Architected Framework Explore advanced features like X-Ray tracing Experiment with Aurora Serverless for database Implement authentication with Amazon Cognito "},{"uri":"https://dokang307.github.io/Internship_Report/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nDuring my internship at Amazon Web Services Vietnam Company Limited from September 8, 2025 to December 28, 2025, I had the opportunity to learn, hone my skills, and apply the academic knowledge I acquired at university to a real-world working environment.\nI participated in 8 office sessions and 6 events. Although I personally feel that the number of activities I attended was relatively limited, these sessions provided me with the opportunity to network with new peers and learn a great deal from them. Additionally, the office visits allowed me to connect with the mentors. I had several interactions with them, but I built the closest connection with Mr. Thinh and Mr. Hoang Anh, with whom I frequently exchange messages.\nThrough these experiences, I have noticed a gradual shift in my mindset. I am transitioning from a student perspective to becoming more mature and adapting to the requirements of corporate culture. I believe this transformation is valuable and essential to prepare myself for my future career.\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ‚úÖ ‚òê ‚òê 2 Ability to learn Ability to absorb new knowledge and learn quickly ‚òê ‚úÖ ‚òê 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ‚úÖ ‚òê ‚òê 4 Sense of responsibility Completing tasks on time and ensuring quality ‚úÖ ‚òê ‚òê 5 Discipline Adhering to schedules, rules, and work processes ‚òê ‚òê ‚úÖ 6 Progressive mindset Willingness to receive feedback and improve oneself ‚òê ‚úÖ ‚òê 7 Communication Presenting ideas and reporting work clearly ‚òê ‚úÖ ‚òê 8 Teamwork Working effectively with colleagues and participating in teams ‚úÖ ‚òê ‚òê 9 Professional conduct Respecting colleagues, partners, and the work environment ‚úÖ ‚òê ‚òê 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ‚òê ‚úÖ ‚òê 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ‚úÖ ‚òê ‚òê 12 Overall General evaluation of the entire internship period ‚úÖ ‚òê ‚òê Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively "},{"uri":"https://dokang307.github.io/Internship_Report/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Goals: Systematize all knowledge acquired in the first half of the program. Deep dive into two main pillars: Security and Resiliency. Prepare a solid foundation for the Midterm Exam. Tasks to be implemented this week: Day Task Start Date Completion Date Reference 2 Midterm Review:\nPart 1: Secure Architecture\n- IAM (Identity \u0026amp; Access Management)\n- KMS (Key Management Service)\n- Security Group \u0026amp; NACLs\n- Secrets Manager\n- GuardDuty (Threat Detection)\n- Shield \u0026amp; WAF (DDoS \u0026amp; Web App Protection)\nPart 2: Resilient Architecture\n- Core metrics to memorize (RTO/RPO)\n- Multi-AZ RDS\n- Disaster Recovery Strategies\n- Auto Scaling and Load Balancing\n- Route 53 and DNS\n- AWS Backup 20/10/2025 20/10/2025 Review Link 3 Self-study: Systematize security services and practice simulation labs 21/10/2025 21/10/2025 4 Self-study: Redraw High Availability (HA) and Disaster Recovery (DR) architecture diagrams 22/10/2025 22/10/2025 5 Take Practice Tests to review knowledge 23/10/2025 23/10/2025 practice 6 Compile questions and review key notes 24/10/2025 24/10/2025 Tips Week 7 achievement: Consolidating Security Mindset and Resilient Architecture Defense in Depth Mindset:\nClearly distinguished the difference and scope between Security Groups (Stateful - Instance level) and NACLs (Stateless - Subnet level). Deeply understood protection at different layers: WAF for applications (Layer 7), Shield for infrastructure (Layer 3/4), and GuardDuty for intelligent threat detection. Mastered the importance of KMS and Secrets Manager in protecting sensitive data \u0026ldquo;at rest\u0026rdquo; and \u0026ldquo;in transit\u0026rdquo;. Resiliency \u0026amp; Recovery:\nMemorized and understood the nature of RTO (Recovery Time Objective) and RPO (Recovery Point Objective) metrics to select appropriate DR strategies (Backup \u0026amp; Restore, Pilot Light, Warm Standby, or Multi-site Active/Active). Mastered Multi-AZ RDS mechanisms for High Availability compared to Read Replicas (used for scaling reads). Understood the coordination between Route 53, ELB, and Auto Scaling to create a flexible, self-healing system capable of handling load effectively. "},{"uri":"https://dokang307.github.io/Internship_Report/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nHere, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don‚Äôt understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n"},{"uri":"https://dokang307.github.io/Internship_Report/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Goals: Consolidate knowledge acquired from previous weeks. Review key architectural patterns (High Performance \u0026amp; Cost Optimization). Complete the Midterm Examination. Tasks to be implemented this week: Day Task Start Date Completion Date Reference 2 Midterm Review:\n1. Part 3: High Performance Architecture:\n- AWS Fargate - Serverless Container\n- AWS Lambda - Event-Driven Architecture\n- Auto Scaling with CloudWatch\n2. Part 4: Cost-optimized Architecture\n- AWS Cost Explorer - Cost Analysis \u0026amp; Optimization\n- S3 Storage Tiering - Lifecycle Automated Cost Optimization\n- Complete VPC Architecture with NAT Gateway 27/10/2025 27/10/2025 Review Link 3 Self-study and consolidation of knowledge 28/10/2025 28/10/2025 4 Self-study and practice tests 29/10/2025 29/10/2025 Practice 5 Final review preparation 30/10/2025 30/10/2025 6 Midterm Exam 31/10/2025 31/10/2025 Week 8 achievement: Consolidating High-Performance \u0026amp; Cost-Optimized Architectures High-Performance Architecture Mastery:\nDeepened understanding of Serverless Containers (AWS Fargate) vs. Event-Driven Compute (AWS Lambda), and when to apply each for optimal performance. Reinforced the mechanics of Auto Scaling triggered by CloudWatch metrics to ensure system stability under varying loads. Cost Optimization Strategies:\nMastered the use of AWS Cost Explorer to analyze spending patterns. Reviewed S3 Storage Tiering and Lifecycle policies to automate cost savings for data storage. Consolidated understanding of networking costs, specifically focusing on NAT Gateways within a complete VPC architecture. Knowledge Validation:\nSuccessfully completed the Midterm Exam, validating the ability to design architectures that balance performance, security, and cost-effectiveness. The exam highlighted areas of strength in networking and areas for further refinement in serverless orchestration. "},{"uri":"https://dokang307.github.io/Internship_Report/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Goals: Build and train the Recommendation Model. Refine data and handle issues arising during the training process. Tasks to be implemented this week: Day Task Start Date Completion Date Reference 2 Model Building:\n- Configure model recipe\n- Create dataset import job\n- Run training process 10/11/2025 11/11/2025 AWS Personalize Docs 3 Preliminary model check and error analysis 12/11/2025 12/11/2025 4 Data Refinement \u0026amp; Update:\n- Add new fields to the Schema\n- Clean and resynchronize the dataset\n- Retrain model with updated data 12/11/2025 13/11/2025 5 Re-evaluate metrics after data update 14/11/2025 14/11/2025 6 Week 9 Harvest: Challenges in Model Training and Data Quality This week focused heavily on model training techniques, and I faced real-world Data Science challenges:\nData Scarcity \u0026amp; Interaction Limitations:\nIssue: The dataset volume is limited, and critically, there is only one interaction type: \u0026ldquo;Booking\u0026rdquo;. The lack of high-funnel interactions like \u0026ldquo;View\u0026rdquo; or \u0026ldquo;Click\u0026rdquo; makes it difficult for the model to capture latent user preferences. Consequence: The model\u0026rsquo;s evaluation metrics after training are relatively low, leading to significant skepticism regarding the accuracy and practical effectiveness of the recommendations. Validation Blockers:\nSince the Website Frontend is not yet complete, I cannot perform real-world validation (visual checks) to see if the recommendations make sense intuitively. Currently, I rely solely on abstract metrics. Data Schema Evolution:\nThe addition of new data fields during development caused friction in the data processing pipeline. Every schema change required re-cleaning the data, re-mapping the schema, and retraining the model from scratch, which was time-consuming. "},{"uri":"https://dokang307.github.io/Internship_Report/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Solution Architecture: Finalize High-Level (HLD) and Low-Level (LLD) architecture diagrams based on the AWS Well-Architected Framework. Resource Planning \u0026amp; Costing: Estimate Total Cost of Ownership (TCO) and select appropriate services. Network Infrastructure Initialization (Networking): Establish the foundation for VPC, Subnets, and basic security policies. Tasks to be implemented this week: Day Task Start Date Completion Date Reference 2-3 Design \u0026amp; Planning:\n- Analyze project business requirements.\n- Draw system architecture diagram.\n- Use AWS Pricing Calculator to create a cost estimate.\n- Review architecture against the 5 pillars of the Well-Architected Framework. 10/11/2025 11/11/2025 4-5 Deploy Network Infrastructure (IaC):\n- Write code (Terraform/CloudFormation) to initialize VPC, Public/Private Subnets, NAT Gateway, Route Tables.\n- Design and configure basic Security Groups for Bastion Host, Web Server, and Database. 12/11/2025 13/11/2025 Week 10 Achievements: Architecture Diagrams \u0026amp; Infrastructure Foundation 1. System Architecture Design Architecture Diagram: Completed data flow diagram and resource layout. The system is designed following the Single-AZ model. Service Selection: Database: RDS MySQL (Multi-AZ) for relational data. Storage: S3 for static assets and backups. Cost Optimization: Created a monthly budget estimate and identified saving strategies (such as using Spot Instances for Dev environments, Savings Plans for Prod). 2. Network Deployment (VPC) Network Structure: Successfully established VPC with custom CIDR block, clearly separating Public Subnets and Private Subnets (for App, DB). Network Security: Established Security Groups following the principle of Least Privilege. "},{"uri":"https://dokang307.github.io/Internship_Report/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Deploy Core Services: Configure Compute and Database resources. Automation (DevOps): Build a CI/CD pipeline to automate code building and deployment to the AWS environment. Tasks to be implemented this week: Day Task Start Date Completion Date Reference 2-3 Compute \u0026amp; DB Configuration:\n- Initialize RDS with Single-AZ.\n- Connect to MySQL. 17/11/2025 18/11/2025 4-5 Build CI/CD Pipeline:\n- Setup Repo on CodeCommit/GitHub.\n- Configure CodeBuild to package the application.\n- Use CodeDeploy/CodePipeline to automatically deploy code to EC2 Instances.\n- Verify the code deployment flow from Local to Cloud. 19/11/2025 20/11/2025 Week 11 Achievements: Operational System \u0026amp; Automated Pipeline 1. High Availability Application Deployment Database: RDS is operating stably in Multi-AZ mode, ensuring data safety in case of failure in one AZ. Successfully connected the App Server to the DB. 2. DevOps Process (CI/CD) Complete Pipeline: Successfully built a CI/CD (Continuous Integration/Continuous Deployment) flow. Source: Code is pushed to GitHub/CodeCommit. Build: Automatically install dependencies and package artifacts. Deploy: Automatically update the new version to all EC2 instances in the ASG without service interruption (Zero Downtime Deployment - using Rolling Update strategy). "},{"uri":"https://dokang307.github.io/Internship_Report/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Security \u0026amp; Monitoring: Scan for security vulnerabilities and set up a system monitoring Dashboard. Project Wrap-up: Finalize documentation, perform final optimizations, and prepare presentation slides. Tasks to be implemented this week: Day Task Start Date Completion Date Reference 2-3 - Configure CloudWatch Alarms and Dashboard.\n- Perform security review using AWS Trusted Advisor and GuardDuty. 24/11/2025 25/11/2025 4-5 Optimization \u0026amp; Packaging:\n- Review and delete redundant resources for cost optimization.\n- Write operational documentation (Runbook).\n- Draft final report slides (Architecture, Challenges, Lessons Learned). 26/11/2025 27/11/2025 Week 12 Achievements: System Finalization \u0026amp; Handover Readiness 1. Performance Evaluation \u0026amp; Tuning Monitoring: Established a CloudWatch Dashboard to visually display key metrics: CPU Utilization, Request Count, Database Connections, Error Rate (4xx, 5xx). Configured SNS Alerts to send email notifications to Admins when system issues occur. 2. Optimization \u0026amp; Advanced Security Security: Enabled WAF (Web Application Firewall) to block common attacks (SQL Injection, XSS). Reviewed IAM Roles and revoked unnecessary privileges. Cost: Based on data from the 2-week pilot run, resized Instances from t3.medium to t3.micro for the Dev environment to save costs without impacting performance. 3. Project Completion Documentation: Completed the project documentation suite including: Updated Architecture Diagram, Redeployment Guide, and Cost Report. Lessons Learned: Gained insights into handling \u0026ldquo;Cold Start\u0026rdquo; issues during Auto Scaling and the importance of accurate Health Check configuration. "},{"uri":"https://dokang307.github.io/Internship_Report/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://dokang307.github.io/Internship_Report/tags/","title":"Tags","tags":[],"description":"","content":""}]